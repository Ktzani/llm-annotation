from typing import List, Dict, Optional, Tuple
from datasets import load_dataset
import pandas as pd

import sys
from pathlib import Path as PathLib

config_path = PathLib(__file__).parent.parent / 'config'
sys.path.insert(0, str(config_path))

from datasets import HUGGINGFACE_DATASETS

def load_hf_dataset(
    dataset_name: str,
    config: Optional[Dict] = None,
    use_cache: bool = True,
    force_reload: bool = False
) -> Tuple[List[str], List[str], Optional[List[str]]]:
    """
    Carrega um dataset do HuggingFace para anotaÃ§Ã£o
    
    Args:
        dataset_name: Nome do dataset em HUGGINGFACE_DATASETS ou path direto
        config: ConfiguraÃ§Ã£o customizada (opcional)
        use_cache: Se True, usa cache do HuggingFace
        force_reload: Se True, recarrega mesmo com cache
    
    Returns:
        Tuple com (texts, categories, ground_truth_labels)
        - texts: Lista de textos para anotar
        - categories: Lista de categorias possÃ­veis
        - ground_truth_labels: Labels verdadeiros (se disponÃ­vel) ou None
    
    Example:
        >>> texts, categories, labels = load_hf_dataset("exemplo_com_labels")
        >>> print(f"Carregados {len(texts)} textos para anotaÃ§Ã£o")
    """
    # Usar config predefinida ou customizada
    if config is None:
        if dataset_name not in HUGGINGFACE_DATASETS:
            raise ValueError(
                f"Dataset '{dataset_name}' nÃ£o encontrado.\n"
                f"Datasets disponÃ­veis: {list(HUGGINGFACE_DATASETS.keys())}\n"
                f"Ou use load_custom_dataset() para carregar diretamente."
            )
        config = HUGGINGFACE_DATASETS[dataset_name]
    
    print(f"\n{'='*80}")
    print(f"ğŸ“¦ Carregando dataset: {dataset_name}")
    print(f"{'='*80}")
    print(f"Path: {config['path']}")
    if 'description' in config:
        print(f"DescriÃ§Ã£o: {config['description']}")
    
    try:
        cache_dir = "./data/.cache/huggingface" if use_cache else None
        
        # Tratar combinaÃ§Ã£o de splits
        if config.get('combine_splits'):
            print(f"\nğŸ”„ Combinando splits: {config['combine_splits']}")
            datasets_list = []
            
            for split in config['combine_splits']:
                try:
                    ds = load_dataset(
                        config['path'],
                        split=split,
                        cache_dir=cache_dir
                    )
                    datasets_list.append(ds)
                    print(f"   âœ“ {split}: {len(ds)} exemplos")
                except Exception as e:
                    print(f"   âš ï¸  {split}: nÃ£o disponÃ­vel ({str(e)})")
            
            if not datasets_list:
                raise ValueError("Nenhum split disponÃ­vel para combinar")
            
            # Concatenar todos os datasets
            from datasets import concatenate_datasets
            dataset = concatenate_datasets(datasets_list)
            print(f"\n   âœ“ Total combinado: {len(dataset)} exemplos")
        
        else:
            # Carregar split Ãºnico
            split = config['split']
            dataset = load_dataset(
                config['path'],
                split=split,
                cache_dir=cache_dir
            )
            print(f"\nâœ“ Split '{split}': {len(dataset)} exemplos")
        
        # Aplicar amostragem se configurado
        if config.get('sample_size') is not None:
            sample_size = min(config['sample_size'], len(dataset))
            dataset = dataset.select(range(sample_size))
            print(f"âœ“ Amostra selecionada: {sample_size} exemplos")
        
        # Extrair textos
        text_column = config['text_column']
        if text_column not in dataset.column_names:
            raise ValueError(
                f"Coluna de texto '{text_column}' nÃ£o encontrada.\n"
                f"Colunas disponÃ­veis: {dataset.column_names}"
            )
        
        texts = dataset[text_column]
        print(f"âœ“ Textos extraÃ­dos da coluna: '{text_column}'")
        
        # Extrair categorias
        categories = config['categories']
        if categories is None:
            # Extrair categorias automaticamente dos labels
            label_column = config.get('label_column')
            if label_column and label_column in dataset.column_names:
                unique_labels = set(dataset[label_column])
                categories = sorted(list(unique_labels))
                print(f"âœ“ Categorias extraÃ­das automaticamente: {categories}")
            else:
                raise ValueError(
                    "Categorias nÃ£o fornecidas e nÃ£o foi possÃ­vel extrair automaticamente.\n"
                    "ForneÃ§a 'categories' na configuraÃ§Ã£o ou 'label_column' com labels vÃ¡lidos."
                )
        else:
            print(f"âœ“ Categorias configuradas: {categories}")
        
        # Extrair ground truth labels (se disponÃ­vel)
        ground_truth = None
        label_column = config.get('label_column')
        if label_column and label_column in dataset.column_names:
            ground_truth = dataset[label_column]
            print(f"âœ“ Ground truth disponÃ­vel (coluna: '{label_column}')")
            print(f"  â†’ Pode ser usado para validaÃ§Ã£o da qualidade das anotaÃ§Ãµes")
        else:
            print(f"â„¹ï¸  Ground truth nÃ£o disponÃ­vel (anotaÃ§Ã£o do zero)")
        
        print(f"\n{'='*80}")
        print(f"âœ… Dataset pronto para anotaÃ§Ã£o!")
        print(f"{'='*80}\n")
        
        return texts, categories, ground_truth
        
    except Exception as e:
        print(f"\nâŒ Erro ao carregar dataset:")
        print(f"   {str(e)}\n")
        raise


def load_hf_dataset_as_dataframe(
    dataset_name: str,
    config: Optional[Dict] = None
) -> pd.DataFrame:
    """
    Carrega dataset e retorna como DataFrame pandas
    
    Args:
        dataset_name: Nome do dataset
        config: ConfiguraÃ§Ã£o customizada (opcional)
    
    Returns:
        DataFrame com colunas: text_id, text, ground_truth (se disponÃ­vel)
    
    Example:
        >>> df = load_hf_dataset_as_dataframe("exemplo_com_labels")
        >>> print(df.head())
    """
    texts, categories, ground_truth = load_hf_dataset(dataset_name, config)
    
    df = pd.DataFrame({
        'text_id': range(len(texts)),
        'text': texts
    })
    
    if ground_truth is not None:
        df['ground_truth'] = ground_truth
    
    return df


def load_custom_dataset(
    hf_path: str,
    text_column: str,
    label_column: Optional[str] = None,
    categories: Optional[List[str]] = None,
    split: str = "train",
    combine_splits: Optional[List[str]] = None,
    sample_size: Optional[int] = None
) -> Tuple[List[str], List[str], Optional[List[str]]]:
    """
    Carrega um dataset personalizado diretamente sem prÃ©-configurar
    
    Args:
        hf_path: Path do dataset no HuggingFace (ex: "waashk/meu-dataset")
        text_column: Nome da coluna com textos
        label_column: Nome da coluna com labels (opcional)
        categories: Lista de categorias (opcional, serÃ¡ extraÃ­da se None)
        split: Split a carregar ("train", "test", etc)
        combine_splits: Lista de splits para combinar (sobrescreve split)
        sample_size: NÃºmero de exemplos a carregar (None = todos)
    
    Returns:
        Tuple com (texts, categories, ground_truth_labels)
    
    Example:
        >>> texts, cats, labels = load_custom_dataset(
        ...     "waashk/meu-dataset",
        ...     text_column="content",
        ...     label_column="category",
        ...     combine_splits=["train", "test"],  # Usar tudo
        ...     sample_size=100
        ... )
    """
    config = {
        'path': hf_path,
        'text_column': text_column,
        'label_column': label_column,
        'categories': categories,
        'split': split,
        'sample_size': sample_size,
    }
    
    if combine_splits:
        config['combine_splits'] = combine_splits
        config['split'] = None
    
    return load_hf_dataset("custom", config)


def list_available_datasets() -> List[str]:
    """
    Lista todos os datasets configurados
    
    Returns:
        Lista de nomes de datasets disponÃ­veis
    """
    return list(HUGGINGFACE_DATASETS.keys())


def get_dataset_info(dataset_name: str) -> Dict:
    """
    Retorna informaÃ§Ãµes sobre um dataset configurado
    
    Args:
        dataset_name: Nome do dataset
    
    Returns:
        DicionÃ¡rio com configuraÃ§Ãµes do dataset
    """
    if dataset_name not in HUGGINGFACE_DATASETS:
        raise ValueError(
            f"Dataset '{dataset_name}' nÃ£o encontrado. "
            f"DisponÃ­veis: {list(HUGGINGFACE_DATASETS.keys())}"
        )
    
    return HUGGINGFACE_DATASETS[dataset_name].copy()


def discover_dataset_structure(hf_path: str, num_examples: int = 3):
    """
    Descobre e exibe a estrutura de um dataset do HuggingFace
    
    Args:
        hf_path: Path do dataset (ex: "waashk/meu-dataset")
        num_examples: NÃºmero de exemplos a mostrar
    
    Example:
        >>> discover_dataset_structure("waashk/meu-dataset")
    """
    print(f"\n{'='*80}")
    print(f"ğŸ” Descobrindo estrutura: {hf_path}")
    print(f"{'='*80}\n")
    
    try:
        # Tentar carregar splits disponÃ­veis
        from datasets import get_dataset_config_names, get_dataset_split_names
        
        try:
            configs = get_dataset_config_names(hf_path)
            print(f"ConfiguraÃ§Ãµes disponÃ­veis: {configs}")
        except:
            print("ConfiguraÃ§Ãµes: [default]")
        
        try:
            splits = get_dataset_split_names(hf_path)
            print(f"Splits disponÃ­veis: {splits}\n")
        except:
            splits = ["train"]
            print(f"Splits disponÃ­veis: {splits} (padrÃ£o)\n")
        
        # Carregar amostra
        dataset = load_dataset(hf_path, split=f"{splits[0]}[:{num_examples}]")
        
        print(f"ğŸ“‹ Estrutura do dataset:")
        print(f"   Colunas: {dataset.column_names}")
        print(f"   Features: {dataset.features}\n")
        
        print(f"ğŸ“ Primeiros {num_examples} exemplos:")
        for i, example in enumerate(dataset):
            print(f"\n   Exemplo {i+1}:")
            for key, value in example.items():
                value_str = str(value)[:100]
                print(f"      {key}: {value_str}...")
        
        print(f"\n{'='*80}")
        print("âœ… Estrutura descoberta!")
        print(f"{'='*80}\n")
        
        # Sugerir configuraÃ§Ã£o
        print("ğŸ’¡ SugestÃ£o de configuraÃ§Ã£o:")
        print(f'''
"seu_dataset": {{
    "path": "{hf_path}",
    "text_column": "{dataset.column_names[0]}",  # AJUSTE SE NECESSÃRIO
    "label_column": None,  # ou nome da coluna de label
    "categories": ["Cat1", "Cat2", "Cat3"],  # DEFINA SUAS CATEGORIAS
    "split": "{splits[0]}",
    "sample_size": 100,  # ComeÃ§ar pequeno
    "description": "Seu dataset para anotaÃ§Ã£o"
}},
''')
        
    except Exception as e:
        print(f"âŒ Erro: {str(e)}\n")


# =============================================================================
# UTILITÃRIOS DE SALVAMENTO
# =============================================================================

def save_annotated_dataset(
    df: pd.DataFrame,
    output_path: str = "./results/annotated_dataset.csv",
    include_ground_truth: bool = True
):
    """
    Salva dataset anotado em formato padronizado
    
    Args:
        df: DataFrame com anotaÃ§Ãµes
        output_path: Caminho para salvar
        include_ground_truth: Se False, remove coluna ground_truth
    """
    df_save = df.copy()
    
    if not include_ground_truth and 'ground_truth' in df_save.columns:
        df_save = df_save.drop(columns=['ground_truth'])
    
    df_save.to_csv(output_path, index=False, encoding='utf-8')
    print(f"âœ… Dataset anotado salvo: {output_path}")


# =============================================================================
# EXEMPLO DE USO
# =============================================================================

if __name__ == "__main__":
    print("\n" + "="*80)
    print(" " * 25 + "DATASET CONFIGURATION")
    print("="*80 + "\n")
    
    # 1. Listar datasets configurados
    print("ğŸ“‹ Datasets configurados:")
    for ds in list_available_datasets():
        info = get_dataset_info(ds)
        desc = info.get('description', 'Sem descriÃ§Ã£o')
        print(f"   â€¢ {ds}: {desc}")
    
    print("\n" + "="*80 + "\n")
    
    # 2. Descobrir estrutura de um dataset
    print("ğŸ” Para descobrir a estrutura de um dataset:")
    print('   discover_dataset_structure("waashk/seu-dataset")\n')
    
    # 3. Carregar um dataset (comentado - descomente para testar)
    # print("ğŸ“¦ Carregando dataset de exemplo...")
    # texts, categories, labels = load_hf_dataset("exemplo_com_labels")
    # print(f"   Textos: {len(texts)}")
    # print(f"   Categorias: {categories}")
    # print(f"   Ground truth: {'Sim' if labels else 'NÃ£o'}")
