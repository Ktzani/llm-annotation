{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anota√ß√£o Autom√°tica de Datasets usando M√∫ltiplas LLMs\n",
    "\n",
    "## An√°lise de Consenso e Valida√ß√£o de Metodologia\n",
    "---\n",
    "\n",
    "### Objetivo\n",
    "\n",
    "Este notebook implementa uma metodologia para reduzir o custo humano na anota√ß√£o de datasets atrav√©s do uso de m√∫ltiplas LLMs e an√°lise de consenso.\n",
    "\n",
    "### Metodologia\n",
    "\n",
    "1. **Anota√ß√£o M√∫ltipla**: 5 LLMs diferentes anotam cada inst√¢ncia\n",
    "2. **Consenso Interno**: Cada LLM anota m√∫ltiplas vezes a mesma inst√¢ncia\n",
    "3. **An√°lise de Consenso**: Calculamos m√©tricas de concord√¢ncia entre LLMs\n",
    "4. **Valida√ß√£o de Par√¢metros**: Testamos se varia√ß√µes nos par√¢metros afetam os resultados\n",
    "5. **Estrat√©gias de Resolu√ß√£o**: Definimos como tratar casos sem consenso claro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup e Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necess√°rios\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Imports dos m√≥dulos customizados\n",
    "from llm_annotator import LLMAnnotator\n",
    "from consensus_analyzer import ConsensusAnalyzer\n",
    "from visualizer import ConsensusVisualizer\n",
    "from config import LLM_CONFIGS, EXPERIMENT_CONFIG\n",
    "\n",
    "# Configura√ß√£o de visualiza√ß√£o\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"‚úì Imports realizados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configura√ß√£o de API Keys\n",
    "\n",
    "**IMPORTANTE:** Configure suas API keys aqui ou use um arquivo .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Op√ß√£o 1: Configura√ß√£o direta (N√ÉO COMMITAR PARA GIT!)\n",
    "api_keys = {\n",
    "    \"openai\": \"sua-api-key-aqui\",\n",
    "    \"anthropic\": \"sua-api-key-aqui\",\n",
    "    \"google\": \"sua-api-key-aqui\",\n",
    "}\n",
    "\n",
    "# Op√ß√£o 2: Carregar de arquivo .env (RECOMENDADO)\n",
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "# load_dotenv()\n",
    "# api_keys = {\n",
    "#     \"openai\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "#     \"anthropic\": os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "#     \"google\": os.getenv(\"GOOGLE_API_KEY\"),\n",
    "# }\n",
    "\n",
    "print(\"‚úì API Keys configuradas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Carregar Dataset\n",
    "\n",
    "Carregue seu dataset aqui. O dataset deve ter pelo menos:\n",
    "- Uma coluna com textos a serem anotados\n",
    "- (Opcional) Uma coluna com labels verdadeiros para valida√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: Carregar dataset\n",
    "# df_dataset = pd.read_csv('seu_dataset.csv')\n",
    "\n",
    "# Para demonstra√ß√£o, vamos criar um dataset de exemplo\n",
    "example_texts = [\n",
    "    \"Este produto √© excelente! Recomendo muito.\",\n",
    "    \"P√©ssima qualidade, n√£o funciona como esperado.\",\n",
    "    \"O produto √© ok, nada de especial.\",\n",
    "    \"Maravilhoso! Superou minhas expectativas.\",\n",
    "    \"Horr√≠vel, totalmente decepcionado.\",\n",
    "]\n",
    "\n",
    "# Categorias dispon√≠veis\n",
    "categories = [\"Positivo\", \"Negativo\", \"Neutro\"]\n",
    "\n",
    "print(f\"Dataset carregado: {len(example_texts)} textos\")\n",
    "print(f\"Categorias: {categories}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configurar Modelos LLM\n",
    "\n",
    "Selecione quais modelos voc√™ deseja usar para anota√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos dispon√≠veis (ajuste conforme suas API keys)\n",
    "selected_models = [\n",
    "    \"gpt-4-turbo\",\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"claude-3-opus\",\n",
    "    \"claude-3-sonnet\",\n",
    "    \"gemini-pro\",\n",
    "]\n",
    "\n",
    "print(f\"Modelos selecionados: {len(selected_models)}\")\n",
    "for model in selected_models:\n",
    "    print(f\"  - {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inicializar Anotador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar o anotador\n",
    "annotator = LLMAnnotator(\n",
    "    models=selected_models,\n",
    "    categories=categories,\n",
    "    api_keys=api_keys,\n",
    "    cache_dir=\"./cache\",\n",
    "    results_dir=\"./results\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Anotador inicializado e pronto para uso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Executar Anota√ß√£o\n",
    "\n",
    "### 6.1 Anota√ß√£o com Par√¢metros Padr√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anotar dataset\n",
    "df_annotations = annotator.annotate_dataset(\n",
    "    texts=example_texts,\n",
    "    num_repetitions=3,  # Cada LLM anota 3 vezes (OBS-2)\n",
    "    test_param_variations=False,  # Mudar para True para testar varia√ß√µes\n",
    ")\n",
    "\n",
    "# Visualizar primeiras linhas\n",
    "print(\"\\nüìä Primeiras anota√ß√µes:\")\n",
    "display(df_annotations.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 (Opcional) Testar Varia√ß√µes de Par√¢metros\n",
    "\n",
    "Investiga se mudan√ßas nos par√¢metros das LLMs afetam significativamente os resultados (\"LLM hacking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomentar para testar varia√ß√µes de par√¢metros\n",
    "# df_annotations_params = annotator.annotate_dataset(\n",
    "#     texts=example_texts,\n",
    "#     num_repetitions=3,\n",
    "#     test_param_variations=True,  # Testa diferentes temperaturas, top_p, etc.\n",
    "# )\n",
    "#\n",
    "# print(\"\\nüìä Anota√ß√µes com varia√ß√µes de par√¢metros:\")\n",
    "# display(df_annotations_params.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Calcular Consenso\n",
    "\n",
    "An√°lise de consenso entre as LLMs e dentro de cada LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular m√©tricas de consenso\n",
    "df_with_consensus = annotator.calculate_consensus(df_annotations)\n",
    "\n",
    "# Visualizar resultados com consenso\n",
    "print(\"\\nüìä Anota√ß√µes com m√©tricas de consenso:\")\n",
    "display(df_with_consensus[[\n",
    "    'text',\n",
    "    'most_common_annotation',\n",
    "    'consensus_score',\n",
    "    'consensus_level',\n",
    "    'unique_annotations',\n",
    "    'is_problematic'\n",
    "]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. An√°lise Detalhada de Consenso\n",
    "\n",
    "### 8.1 M√©tricas de Dist√¢ncia e Concord√¢ncia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar analisador\n",
    "analyzer = ConsensusAnalyzer(categories=categories)\n",
    "\n",
    "# Coletar colunas de consenso\n",
    "consensus_cols = [col for col in df_with_consensus.columns if '_consensus' in col and '_score' not in col]\n",
    "\n",
    "# Gerar relat√≥rio completo\n",
    "report = analyzer.generate_consensus_report(\n",
    "    df=df_with_consensus,\n",
    "    annotator_cols=consensus_cols,\n",
    "    output_dir=\"./results\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Relat√≥rio de consenso gerado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Visualizar M√©tricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibir m√©tricas calculadas\n",
    "print(\"\\nüìä M√âTRICAS DE DIST√ÇNCIA E CONCORD√ÇNCIA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for metric, value in report['distance_metrics'].items():\n",
    "    if value is not None:\n",
    "        print(f\"{metric:30s}: {value:8.4f}\")\n",
    "\n",
    "# Interpreta√ß√£o das m√©tricas\n",
    "print(\"\\nüí° INTERPRETA√á√ÉO:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if 'mean_cohen_kappa' in report['distance_metrics']:\n",
    "    kappa = report['distance_metrics']['mean_cohen_kappa']\n",
    "    if kappa > 0.8:\n",
    "        print(\"‚úì Cohen's Kappa > 0.8: Concord√¢ncia EXCELENTE\")\n",
    "    elif kappa > 0.6:\n",
    "        print(\"‚úì Cohen's Kappa > 0.6: Concord√¢ncia BOA\")\n",
    "    elif kappa > 0.4:\n",
    "        print(\"‚ö† Cohen's Kappa > 0.4: Concord√¢ncia MODERADA\")\n",
    "    else:\n",
    "        print(\"‚ö† Cohen's Kappa < 0.4: Concord√¢ncia FRACA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Matriz de Concord√¢ncia Par a Par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar matriz de concord√¢ncia\n",
    "print(\"\\nüìä Matriz de Concord√¢ncia Par a Par:\")\n",
    "display(report['pairwise_agreement'])\n",
    "\n",
    "print(\"\\nüìä Matriz de Cohen's Kappa Par a Par:\")\n",
    "display(report['pairwise_kappa'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. An√°lise de Inst√¢ncias Problem√°ticas\n",
    "\n",
    "Identificar casos onde h√° discord√¢ncia significativa entre as LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar inst√¢ncias problem√°ticas\n",
    "problematic = df_with_consensus[df_with_consensus['is_problematic'] == True]\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è INST√ÇNCIAS PROBLEM√ÅTICAS: {len(problematic)}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(problematic) > 0:\n",
    "    for idx, row in problematic.iterrows():\n",
    "        print(f\"\\nTexto {idx}:\")\n",
    "        print(f\"  {row['text'][:100]}...\")\n",
    "        print(f\"  Anota√ß√µes: {row['all_annotations']}\")\n",
    "        print(f\"  Consenso: {row['consensus_score']:.2%}\")\n",
    "        print(f\"  Categoria mais comum: {row['most_common_annotation']}\")\n",
    "        print(\"-\" * 80)\n",
    "else:\n",
    "    print(\"‚úì Nenhuma inst√¢ncia problem√°tica encontrada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Estrat√©gias para Resolver Casos Problem√°ticos\n",
    "\n",
    "Aqui implementamos diferentes estrat√©gias para lidar com casos sem consenso claro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_conflicts(df, strategy='majority_vote', threshold=0.6):\n",
    "    \"\"\"\n",
    "    Resolve conflitos usando diferentes estrat√©gias\n",
    "    \n",
    "    Estrat√©gias:\n",
    "    - 'majority_vote': Usa voto majorit√°rio simples\n",
    "    - 'unanimous_only': Aceita apenas casos com 100% consenso\n",
    "    - 'threshold': Aceita apenas se consenso >= threshold\n",
    "    - 'flag_for_review': Marca para revis√£o humana\n",
    "    - 'remove': Remove inst√¢ncias problem√°ticas\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    if strategy == 'majority_vote':\n",
    "        df['final_annotation'] = df['most_common_annotation']\n",
    "        df['needs_review'] = False\n",
    "        \n",
    "    elif strategy == 'unanimous_only':\n",
    "        df['final_annotation'] = df.apply(\n",
    "            lambda row: row['most_common_annotation'] if row['consensus_score'] == 1.0 else None,\n",
    "            axis=1\n",
    "        )\n",
    "        df['needs_review'] = df['final_annotation'].isna()\n",
    "        \n",
    "    elif strategy == 'threshold':\n",
    "        df['final_annotation'] = df.apply(\n",
    "            lambda row: row['most_common_annotation'] if row['consensus_score'] >= threshold else None,\n",
    "            axis=1\n",
    "        )\n",
    "        df['needs_review'] = df['final_annotation'].isna()\n",
    "        \n",
    "    elif strategy == 'flag_for_review':\n",
    "        df['final_annotation'] = df['most_common_annotation']\n",
    "        df['needs_review'] = df['is_problematic'] | (df['consensus_score'] < threshold)\n",
    "        \n",
    "    elif strategy == 'remove':\n",
    "        df = df[~df['is_problematic'] & (df['consensus_score'] >= threshold)].copy()\n",
    "        df['final_annotation'] = df['most_common_annotation']\n",
    "        df['needs_review'] = False\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Testar diferentes estrat√©gias\n",
    "strategies = ['majority_vote', 'threshold', 'flag_for_review']\n",
    "\n",
    "print(\"\\nüìä COMPARA√á√ÉO DE ESTRAT√âGIAS DE RESOLU√á√ÉO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for strategy in strategies:\n",
    "    df_resolved = resolve_conflicts(df_with_consensus, strategy=strategy, threshold=0.6)\n",
    "    \n",
    "    print(f\"\\nEstrat√©gia: {strategy}\")\n",
    "    print(f\"  Total de inst√¢ncias: {len(df_resolved)}\")\n",
    "    print(f\"  Anota√ß√µes finais: {df_resolved['final_annotation'].notna().sum()}\")\n",
    "    print(f\"  Necessitam revis√£o: {df_resolved['needs_review'].sum()}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualiza√ß√µes\n",
    "\n",
    "### 10.1 Gerar Todas as Visualiza√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar visualizador\n",
    "visualizer = ConsensusVisualizer(output_dir=\"./results/figures\")\n",
    "\n",
    "# Gerar visualiza√ß√µes\n",
    "print(\"Gerando visualiza√ß√µes...\\n\")\n",
    "\n",
    "# 1. Heatmap de concord√¢ncia\n",
    "visualizer.plot_agreement_heatmap(\n",
    "    report['pairwise_agreement'],\n",
    "    title=\"Concord√¢ncia entre Modelos LLM\"\n",
    ")\n",
    "\n",
    "# 2. Distribui√ß√£o de consenso\n",
    "visualizer.plot_consensus_distribution(df_with_consensus)\n",
    "\n",
    "# 3. Matriz de confus√£o\n",
    "if 'disagreement_patterns' in report:\n",
    "    visualizer.plot_confusion_matrix(\n",
    "        report['disagreement_patterns']['confusion_matrix']\n",
    "    )\n",
    "\n",
    "# 4. Compara√ß√£o entre modelos\n",
    "visualizer.plot_model_comparison(\n",
    "    df_with_consensus,\n",
    "    models=selected_models\n",
    ")\n",
    "\n",
    "# 5. Dashboard interativo\n",
    "visualizer.create_interactive_dashboard(\n",
    "    df_with_consensus,\n",
    "    report\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Todas as visualiza√ß√µes foram geradas!\")\n",
    "print(\"üìÅ Arquivos salvos em: ./results/figures/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Visualizar no Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "# Exibir algumas visualiza√ß√µes inline\n",
    "print(\"üìä Heatmap de Concord√¢ncia:\")\n",
    "display(Image(filename='./results/figures/agreement_heatmap.png'))\n",
    "\n",
    "print(\"\\nüìä Distribui√ß√£o de Consenso:\")\n",
    "display(Image(filename='./results/figures/consensus_distribution.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. An√°lise de Impacto de Par√¢metros\n",
    "\n",
    "Se voc√™ testou varia√ß√µes de par√¢metros, analise o impacto aqui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomentar se voc√™ testou varia√ß√µes de par√¢metros\n",
    "# for model in selected_models:\n",
    "#     param_cols = [col for col in df_annotations_params.columns if col.startswith(f\"{model}_param_var\")]\n",
    "#     if param_cols:\n",
    "#         visualizer.plot_parameter_impact(\n",
    "#             df_annotations_params,\n",
    "#             model=model\n",
    "#         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Sum√°rio e Recomenda√ß√µes\n",
    "\n",
    "### 12.1 Estat√≠sticas Finais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" \" * 25 + \"SUM√ÅRIO FINAL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Estat√≠sticas gerais\n",
    "print(\"\\nüìä ESTAT√çSTICAS GERAIS:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Total de inst√¢ncias anotadas: {len(df_with_consensus)}\")\n",
    "print(f\"Modelos utilizados: {len(selected_models)}\")\n",
    "print(f\"Repeti√ß√µes por modelo: {EXPERIMENT_CONFIG['num_repetitions_per_llm']}\")\n",
    "\n",
    "# Consenso\n",
    "print(\"\\nüìä DISTRIBUI√á√ÉO DE CONSENSO:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Alto consenso (‚â•80%): {(df_with_consensus['consensus_score'] >= 0.8).sum()} ({(df_with_consensus['consensus_score'] >= 0.8).sum() / len(df_with_consensus) * 100:.1f}%)\")\n",
    "print(f\"M√©dio consenso (60-80%): {((df_with_consensus['consensus_score'] >= 0.6) & (df_with_consensus['consensus_score'] < 0.8)).sum()} ({((df_with_consensus['consensus_score'] >= 0.6) & (df_with_consensus['consensus_score'] < 0.8)).sum() / len(df_with_consensus) * 100:.1f}%)\")\n",
    "print(f\"Baixo consenso (<60%): {(df_with_consensus['consensus_score'] < 0.6).sum()} ({(df_with_consensus['consensus_score'] < 0.6).sum() / len(df_with_consensus) * 100:.1f}%)\")\n",
    "\n",
    "# Casos problem√°ticos\n",
    "print(\"\\n‚ö†Ô∏è  CASOS PROBLEM√ÅTICOS:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Total: {df_with_consensus['is_problematic'].sum()} ({df_with_consensus['is_problematic'].sum() / len(df_with_consensus) * 100:.1f}%)\")\n",
    "\n",
    "# M√©tricas de concord√¢ncia\n",
    "print(\"\\nüìä M√âTRICAS DE CONCORD√ÇNCIA:\")\n",
    "print(\"-\" * 80)\n",
    "if 'mean_cohen_kappa' in report['distance_metrics']:\n",
    "    print(f\"Cohen's Kappa m√©dio: {report['distance_metrics']['mean_cohen_kappa']:.4f}\")\n",
    "if 'fleiss_kappa' in report['distance_metrics'] and report['distance_metrics']['fleiss_kappa']:\n",
    "    print(f\"Fleiss' Kappa: {report['distance_metrics']['fleiss_kappa']:.4f}\")\n",
    "if 'mean_jaccard_similarity' in report['distance_metrics']:\n",
    "    print(f\"Jaccard Similarity m√©dio: {report['distance_metrics']['mean_jaccard_similarity']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2 Recomenda√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüí° RECOMENDA√á√ïES:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Baseado no consenso m√©dio\n",
    "consensus_mean = df_with_consensus['consensus_score'].mean()\n",
    "\n",
    "if consensus_mean >= 0.8:\n",
    "    print(\"\\n‚úì EXCELENTE! O consenso entre as LLMs √© muito alto.\")\n",
    "    print(\"  Recomenda√ß√£o: As anota√ß√µes autom√°ticas s√£o confi√°veis para a maioria dos casos.\")\n",
    "    print(\"  Sugest√£o: Revisar apenas os casos marcados como problem√°ticos.\")\n",
    "elif consensus_mean >= 0.6:\n",
    "    print(\"\\n‚ö† BOM. O consenso √© satisfat√≥rio, mas h√° espa√ßo para melhoria.\")\n",
    "    print(\"  Recomenda√ß√£o: Considere revisar casos com consenso < 70%.\")\n",
    "    print(\"  Sugest√£o: Testar prompts alternativos ou adicionar exemplos (few-shot).\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è ATEN√á√ÉO. O consenso entre as LLMs √© baixo.\")\n",
    "    print(\"  Recomenda√ß√£o: Revis√£o humana extensiva √© necess√°ria.\")\n",
    "    print(\"  Sugest√£o: Revisar as categorias e melhorar os prompts.\")\n",
    "\n",
    "# Baseado em casos problem√°ticos\n",
    "problematic_ratio = df_with_consensus['is_problematic'].sum() / len(df_with_consensus)\n",
    "\n",
    "if problematic_ratio > 0.2:\n",
    "    print(\"\\n‚ö†Ô∏è ATEN√á√ÉO: Mais de 20% dos casos s√£o problem√°ticos.\")\n",
    "    print(\"  Sugest√£o: Considere refinar as categorias ou usar prompts mais espec√≠ficos.\")\n",
    "\n",
    "# Baseado em Cohen's Kappa\n",
    "if 'mean_cohen_kappa' in report['distance_metrics']:\n",
    "    kappa = report['distance_metrics']['mean_cohen_kappa']\n",
    "    if kappa < 0.6:\n",
    "        print(\"\\n‚ö†Ô∏è Cohen's Kappa indica concord√¢ncia moderada ou fraca.\")\n",
    "        print(\"  Sugest√£o: Considere:\")\n",
    "        print(\"    1. Adicionar exemplos (few-shot learning)\")\n",
    "        print(\"    2. Melhorar defini√ß√µes de categorias\")\n",
    "        print(\"    3. Usar modelos mais avan√ßados\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Exportar Resultados\n",
    "\n",
    "Exportar todos os resultados para an√°lise posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar diret√≥rio de resultados finais\n",
    "final_results_dir = Path(\"./results/final\")\n",
    "final_results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Exportar dataset anotado completo\n",
    "df_with_consensus.to_csv(\n",
    "    final_results_dir / \"annotated_dataset_complete.csv\",\n",
    "    index=False,\n",
    "    encoding='utf-8'\n",
    ")\n",
    "\n",
    "# Exportar apenas anota√ß√µes de alta confian√ßa\n",
    "high_confidence = df_with_consensus[df_with_consensus['consensus_score'] >= 0.8].copy()\n",
    "high_confidence[['text', 'final_annotation', 'consensus_score']].to_csv(\n",
    "    final_results_dir / \"high_confidence_annotations.csv\",\n",
    "    index=False,\n",
    "    encoding='utf-8'\n",
    ")\n",
    "\n",
    "# Exportar casos para revis√£o\n",
    "needs_review = df_with_consensus[\n",
    "    (df_with_consensus['consensus_score'] < 0.6) | (df_with_consensus['is_problematic'] == True)\n",
    "].copy()\n",
    "needs_review.to_csv(\n",
    "    final_results_dir / \"needs_human_review.csv\",\n",
    "    index=False,\n",
    "    encoding='utf-8'\n",
    ")\n",
    "\n",
    "# Exportar sum√°rio em JSON\n",
    "import json\n",
    "\n",
    "summary = {\n",
    "    \"experiment_config\": EXPERIMENT_CONFIG,\n",
    "    \"models_used\": selected_models,\n",
    "    \"categories\": categories,\n",
    "    \"total_instances\": len(df_with_consensus),\n",
    "    \"consensus_statistics\": {\n",
    "        \"mean\": float(df_with_consensus['consensus_score'].mean()),\n",
    "        \"median\": float(df_with_consensus['consensus_score'].median()),\n",
    "        \"std\": float(df_with_consensus['consensus_score'].std()),\n",
    "    },\n",
    "    \"distribution\": {\n",
    "        \"high_consensus\": int((df_with_consensus['consensus_score'] >= 0.8).sum()),\n",
    "        \"medium_consensus\": int(((df_with_consensus['consensus_score'] >= 0.6) & (df_with_consensus['consensus_score'] < 0.8)).sum()),\n",
    "        \"low_consensus\": int((df_with_consensus['consensus_score'] < 0.6).sum()),\n",
    "    },\n",
    "    \"problematic_instances\": int(df_with_consensus['is_problematic'].sum()),\n",
    "    \"distance_metrics\": {k: float(v) if v is not None else None for k, v in report['distance_metrics'].items()},\n",
    "}\n",
    "\n",
    "with open(final_results_dir / \"experiment_summary.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"\\n‚úì Resultados exportados com sucesso!\")\n",
    "print(f\"üìÅ Diret√≥rio: {final_results_dir}\")\n",
    "print(\"\\nArquivos gerados:\")\n",
    "print(\"  - annotated_dataset_complete.csv: Dataset completo com todas as anota√ß√µes\")\n",
    "print(\"  - high_confidence_annotations.csv: Apenas anota√ß√µes de alta confian√ßa\")\n",
    "print(\"  - needs_human_review.csv: Casos que precisam de revis√£o humana\")\n",
    "print(\"  - experiment_summary.json: Sum√°rio do experimento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Conclus√µes e Pr√≥ximos Passos\n",
    "\n",
    "### Principais Achados:\n",
    "\n",
    "1. **Consenso entre LLMs**: [A ser preenchido ap√≥s an√°lise]\n",
    "2. **Impacto de par√¢metros**: [A ser preenchido ap√≥s an√°lise]\n",
    "3. **Casos problem√°ticos**: [A ser preenchido ap√≥s an√°lise]\n",
    "4. **Economia de custo**: [Calcular percentual de inst√¢ncias que n√£o precisam revis√£o humana]\n",
    "\n",
    "### Pr√≥ximos Passos:\n",
    "\n",
    "1. [ ] Validar anota√ß√µes autom√°ticas com ground truth (se dispon√≠vel)\n",
    "2. [ ] Testar com diferentes prompts (few-shot, Chain-of-Thought)\n",
    "3. [ ] Escalar para datasets maiores\n",
    "4. [ ] Implementar sistema de revis√£o humana para casos problem√°ticos\n",
    "5. [ ] Documentar custos de API e tempo de execu√ß√£o\n",
    "6. [ ] Apresentar resultados para orientador\n",
    "\n",
    "### Material para Discuss√£o com Orientador:\n",
    "\n",
    "- Este notebook completo com an√°lises\n",
    "- Visualiza√ß√µes geradas em `./results/figures/`\n",
    "- Dashboard interativo em `./results/figures/interactive_dashboard.html`\n",
    "- Sum√°rio do experimento em `./results/final/experiment_summary.json`\n",
    "- Matriz de concord√¢ncia e m√©tricas de dist√¢ncia"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
