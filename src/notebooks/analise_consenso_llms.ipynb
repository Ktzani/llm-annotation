{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ An√°lise de Consenso entre LLMs\n",
    "## Notebook Refatorado\n",
    "\n",
    "Este notebook usa a estrutura refatorada com:\n",
    "- Componentes modulares\n",
    "- Logging com loguru\n",
    "- Integra√ß√£o com HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup e Configura√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m14:59:29\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1m‚úì Setup completo\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "\n",
    "# Configurar logging\n",
    "logger.remove()\n",
    "logger.add(\n",
    "    sys.stdout,\n",
    "    format=\"<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>\",\n",
    "    level=\"INFO\"\n",
    ")\n",
    "\n",
    "# Paths\n",
    "project_root = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "sys.path.insert(0, str(project_root / 'src' / 'llm_annotation_system'))\n",
    "sys.path.insert(0, str(project_root / 'src' / 'config'))\n",
    "sys.path.insert(0, str(project_root / 'src' / 'utils'))\n",
    "\n",
    "logger.success(\"‚úì Setup completo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Carregar Dataset do HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_loader'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_hf_dataset, list_available_datasets\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Listar datasets\u001b[39;00m\n\u001b[32m      4\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mDatasets dispon√≠veis:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'data_loader'"
     ]
    }
   ],
   "source": [
    "from src.utils.data_loader import load_hf_dataset, list_available_datasets\n",
    "\n",
    "# Listar datasets\n",
    "logger.info(\"Datasets dispon√≠veis:\")\n",
    "for dataset in list_available_datasets():\n",
    "    logger.info(f\"  - {dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dataset\n",
    "dataset_name = \"agnews\"  # Ajuste conforme necess√°rio\n",
    "\n",
    "texts, categories, ground_truth = load_hf_dataset(dataset_name)\n",
    "\n",
    "logger.info(f\"Textos: {len(texts)}\")\n",
    "logger.info(f\"Categorias: {categories}\")\n",
    "logger.info(f\"Ground truth: {'Sim' if ground_truth else 'N√£o'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar amostra\n",
    "logger.info(\"\\nAmostra dos textos:\")\n",
    "for i, text in enumerate(texts[:3]):\n",
    "    logger.info(f\"{i+1}. {text[:100]}...\")\n",
    "    if ground_truth:\n",
    "        logger.info(f\"   Label: {ground_truth[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Configurar Modelos LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_annotation_system.annotation.llm_annotator import LLMAnnotator\n",
    "\n",
    "# Modelos (ajuste conforme dispon√≠vel)\n",
    "models = [\n",
    "    \"llama3-8b\",\n",
    "    \"mistral-7b\",\n",
    "    \"qwen2-7b\",\n",
    "]\n",
    "\n",
    "# Inicializar\n",
    "annotator = LLMAnnotator(\n",
    "    models=models,\n",
    "    categories=categories,\n",
    "    api_keys=None,  # Ou configure com suas keys\n",
    "    use_langchain_cache=True\n",
    ")\n",
    "\n",
    "logger.success(\"‚úì Annotator inicializado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Executar Anota√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Par√¢metros\n",
    "num_repetitions = 3\n",
    "\n",
    "# Estimativa\n",
    "total_annotations = len(texts) * len(models) * num_repetitions\n",
    "logger.info(f\"Total de anota√ß√µes: {total_annotations}\")\n",
    "\n",
    "# Anotar\n",
    "df_annotations = annotator.annotate_dataset(\n",
    "    texts=texts,\n",
    "    num_repetitions=num_repetitions\n",
    ")\n",
    "\n",
    "logger.success(\"‚úì Anota√ß√µes completas\")\n",
    "display(df_annotations.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Calcular Consenso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular consenso\n",
    "df_with_consensus = annotator.calculate_consensus(df_annotations)\n",
    "\n",
    "# Estat√≠sticas\n",
    "logger.info(\"\\nüìä Estat√≠sticas de Consenso:\")\n",
    "logger.info(f\"  M√©dia: {df_with_consensus['consensus_score'].mean():.2%}\")\n",
    "logger.info(f\"  Mediana: {df_with_consensus['consensus_score'].median():.2%}\")\n",
    "logger.info(f\"  Desvio padr√£o: {df_with_consensus['consensus_score'].std():.2%}\")\n",
    "\n",
    "# Distribui√ß√£o por n√≠vel\n",
    "levels = df_with_consensus['consensus_level'].value_counts()\n",
    "logger.info(\"\\nDistribui√ß√£o por n√≠vel:\")\n",
    "for level, count in levels.items():\n",
    "    logger.info(f\"  {level}: {count} ({count/len(df_with_consensus):.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√µes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histograma\n",
    "axes[0].hist(df_with_consensus['consensus_score'], bins=20, edgecolor='black')\n",
    "axes[0].set_xlabel('Consensus Score')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribui√ß√£o de Scores de Consenso')\n",
    "\n",
    "# Barras por n√≠vel\n",
    "levels.plot(kind='bar', ax=axes[1], color=['green', 'orange', 'red'])\n",
    "axes[1].set_xlabel('N√≠vel de Consenso')\n",
    "axes[1].set_ylabel('Contagem')\n",
    "axes[1].set_title('Casos por N√≠vel de Consenso')\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ An√°lise Detalhada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from consensus_analyzer_refactored import ConsensusAnalyzer\n",
    "\n",
    "# Inicializar analyzer\n",
    "analyzer = ConsensusAnalyzer(categories)\n",
    "\n",
    "# Colunas de consenso\n",
    "consensus_cols = [col for col in df_with_consensus.columns if '_consensus' in col and '_score' not in col]\n",
    "\n",
    "# Gerar relat√≥rio\n",
    "report = analyzer.generate_consensus_report(\n",
    "    df=df_with_consensus,\n",
    "    annotator_cols=consensus_cols,\n",
    "    output_dir=\"./results\"\n",
    ")\n",
    "\n",
    "logger.success(\"‚úì Relat√≥rio gerado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©tricas\n",
    "logger.info(\"\\nüìä M√©tricas de Dist√¢ncia:\")\n",
    "logger.info(f\"  Fleiss' Kappa: {report['fleiss_kappa']:.3f} ({report['fleiss_interpretation']})\")\n",
    "\n",
    "# Interpreta√ß√£o\n",
    "kappa = report['fleiss_kappa']\n",
    "if kappa > 0.8:\n",
    "    logger.success(\"Concord√¢ncia excelente!\")\n",
    "elif kappa > 0.6:\n",
    "    logger.info(\"Concord√¢ncia boa\")\n",
    "elif kappa > 0.4:\n",
    "    logger.warning(\"Concord√¢ncia moderada\")\n",
    "else:\n",
    "    logger.warning(\"Concord√¢ncia fraca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de concord√¢ncia\n",
    "agreement_df = report['pairwise_agreement']\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(agreement_df, annot=True, fmt='.2f', cmap='YlGnBu', cbar_kws={'label': 'Agreement'})\n",
    "plt.title('Matriz de Concord√¢ncia Par a Par')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casos problem√°ticos\n",
    "problematic = report.get('problematic_cases')\n",
    "if problematic is not None and len(problematic) > 0:\n",
    "    logger.warning(f\"\\n‚ö†Ô∏è  {len(problematic)} casos problem√°ticos identificados\")\n",
    "    display(problematic.head())\n",
    "else:\n",
    "    logger.success(\"\\n‚úì Nenhum caso problem√°tico identificado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Valida√ß√£o com Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ground_truth:\n",
    "    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "    \n",
    "    # Adicionar ground truth\n",
    "    df_with_consensus['ground_truth'] = ground_truth\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(\n",
    "        df_with_consensus['ground_truth'],\n",
    "        df_with_consensus['most_common_annotation']\n",
    "    )\n",
    "    \n",
    "    logger.success(f\"\\nüéØ Accuracy: {accuracy:.2%}\")\n",
    "    \n",
    "    # Classification report\n",
    "    logger.info(\"\\nClassification Report:\")\n",
    "    print(classification_report(\n",
    "        df_with_consensus['ground_truth'],\n",
    "        df_with_consensus['most_common_annotation']\n",
    "    ))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(\n",
    "        df_with_consensus['ground_truth'],\n",
    "        df_with_consensus['most_common_annotation']\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=categories, yticklabels=categories)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix vs Ground Truth')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./results/confusion_vs_ground_truth.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    # Accuracy por n√≠vel de consenso\n",
    "    logger.info(\"\\nAccuracy por n√≠vel de consenso:\")\n",
    "    for level in ['high', 'medium', 'low']:\n",
    "        df_level = df_with_consensus[df_with_consensus['consensus_level'] == level]\n",
    "        if len(df_level) > 0:\n",
    "            acc_level = accuracy_score(\n",
    "                df_level['ground_truth'],\n",
    "                df_level['most_common_annotation']\n",
    "            )\n",
    "            logger.info(f\"  {level}: {acc_level:.2%} ({len(df_level)} casos)\")\n",
    "else:\n",
    "    logger.info(\"\\n‚ö†Ô∏è  Ground truth n√£o dispon√≠vel - pulando valida√ß√£o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Exportar Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Criar diret√≥rio\n",
    "results_dir = Path('./results/final')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Salvar CSVs\n",
    "df_with_consensus.to_csv(results_dir / 'dataset_anotado_completo.csv', index=False)\n",
    "logger.info(f\"‚úì Salvos: {len(df_with_consensus)} registros\")\n",
    "\n",
    "# Alta confian√ßa\n",
    "high_conf = df_with_consensus[df_with_consensus['consensus_score'] >= 0.8]\n",
    "high_conf.to_csv(results_dir / 'alta_confianca.csv', index=False)\n",
    "logger.info(f\"‚úì Alta confian√ßa: {len(high_conf)} registros\")\n",
    "\n",
    "# Necessita revis√£o\n",
    "low_conf = df_with_consensus[df_with_consensus['consensus_score'] < 0.8]\n",
    "low_conf.to_csv(results_dir / 'necessita_revisao.csv', index=False)\n",
    "logger.info(f\"‚úì Necessita revis√£o: {len(low_conf)} registros\")\n",
    "\n",
    "# Sum√°rio JSON\n",
    "summary = {\n",
    "    'dataset': {\n",
    "        'name': dataset_name,\n",
    "        'total_texts': len(texts),\n",
    "        'categories': categories,\n",
    "        'has_ground_truth': ground_truth is not None\n",
    "    },\n",
    "    'config': {\n",
    "        'models': models,\n",
    "        'num_repetitions': num_repetitions,\n",
    "        'total_annotations': total_annotations\n",
    "    },\n",
    "    'results': {\n",
    "        'consensus_mean': float(df_with_consensus['consensus_score'].mean()),\n",
    "        'consensus_median': float(df_with_consensus['consensus_score'].median()),\n",
    "        'consensus_std': float(df_with_consensus['consensus_score'].std()),\n",
    "        'high_consensus': int((df_with_consensus['consensus_level'] == 'high').sum()),\n",
    "        'medium_consensus': int((df_with_consensus['consensus_level'] == 'medium').sum()),\n",
    "        'low_consensus': int((df_with_consensus['consensus_level'] == 'low').sum()),\n",
    "        'problematic': int(df_with_consensus['is_problematic'].sum())\n",
    "    },\n",
    "    'metrics': {\n",
    "        'fleiss_kappa': float(report['fleiss_kappa']),\n",
    "        'fleiss_interpretation': report['fleiss_interpretation']\n",
    "    }\n",
    "}\n",
    "\n",
    "if ground_truth:\n",
    "    summary['validation'] = {\n",
    "        'accuracy': float(accuracy)\n",
    "    }\n",
    "\n",
    "with open(results_dir / 'sumario_experimento.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "logger.success(\"\\n‚úì Resultados exportados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Resumo Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.success(\"RESUMO DO EXPERIMENTO\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "logger.info(f\"\\nüìä Dataset: {dataset_name}\")\n",
    "logger.info(f\"  Textos: {len(texts)}\")\n",
    "logger.info(f\"  Categorias: {len(categories)}\")\n",
    "\n",
    "logger.info(f\"\\nü§ñ Configura√ß√£o:\")\n",
    "logger.info(f\"  Modelos: {len(models)}\")\n",
    "logger.info(f\"  Repeti√ß√µes: {num_repetitions}\")\n",
    "logger.info(f\"  Total de anota√ß√µes: {total_annotations}\")\n",
    "\n",
    "logger.info(f\"\\nüìà Consenso:\")\n",
    "logger.info(f\"  M√©dia: {df_with_consensus['consensus_score'].mean():.2%}\")\n",
    "logger.info(f\"  Fleiss' Kappa: {report['fleiss_kappa']:.3f} ({report['fleiss_interpretation']})\")\n",
    "\n",
    "if ground_truth:\n",
    "    logger.info(f\"\\nüéØ Valida√ß√£o:\")\n",
    "    logger.info(f\"  Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "logger.info(f\"\\nüìÅ Arquivos gerados:\")\n",
    "logger.info(f\"  {results_dir}/dataset_anotado_completo.csv\")\n",
    "logger.info(f\"  {results_dir}/alta_confianca.csv\")\n",
    "logger.info(f\"  {results_dir}/necessita_revisao.csv\")\n",
    "logger.info(f\"  {results_dir}/sumario_experimento.json\")\n",
    "\n",
    "cache_stats = annotator.get_cache_stats()\n",
    "logger.info(f\"\\nüíæ Cache: {cache_stats['total_entries']} entradas\")\n",
    "\n",
    "logger.success(\"\\n‚úÖ An√°lise completa!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-annotation-py3.11 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
