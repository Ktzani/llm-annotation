{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ An√°lise de Consenso entre LLMs\n",
    "## Notebook Refatorado com Alternative Params\n",
    "\n",
    "Este notebook usa:\n",
    "- Componentes modulares\n",
    "- Logging com loguru\n",
    "- Integra√ß√£o com HuggingFace\n",
    "- **Alternative params** para testar varia√ß√µes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup e Configura√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m17:21:30\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[32m\u001b[1m‚úì Setup completo\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "import os\n",
    "\n",
    "# Configurar logging\n",
    "logger.remove()\n",
    "logger.add(\n",
    "    sys.stdout,\n",
    "    format=\"<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>\",\n",
    "    level=\"INFO\"\n",
    ")\n",
    "\n",
    "logger.success(\"‚úì Setup completo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Carregar Dataset do HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m17:21:31\u001b[0m | \u001b[1mDatasets dispon√≠veis:\u001b[0m\n",
      "\u001b[32m17:21:31\u001b[0m | \u001b[1m  - agnews\u001b[0m\n",
      "\u001b[32m17:21:31\u001b[0m | \u001b[1m  - mpqa\u001b[0m\n",
      "\u001b[32m17:21:31\u001b[0m | \u001b[1m  - webkb\u001b[0m\n",
      "\u001b[32m17:21:31\u001b[0m | \u001b[1m  - ohsumed\u001b[0m\n",
      "\u001b[32m17:21:31\u001b[0m | \u001b[1m  - acm\u001b[0m\n",
      "\u001b[32m17:21:31\u001b[0m | \u001b[1m  - yelp_2013\u001b[0m\n",
      "\u001b[32m17:21:31\u001b[0m | \u001b[1m  - dblp\u001b[0m\n",
      "\u001b[32m17:21:31\u001b[0m | \u001b[1m  - books\u001b[0m\n",
      "\u001b[32m17:21:31\u001b[0m | \u001b[1m  - reut90\u001b[0m\n",
      "\u001b[32m17:21:31\u001b[0m | \u001b[1m  - wos11967\u001b[0m\n",
      "\u001b[32m17:21:31\u001b[0m | \u001b[1m  - twitter\u001b[0m\n",
      "\u001b[32m17:21:31\u001b[0m | \u001b[1m  - trec\u001b[0m\n",
      "\u001b[32m17:21:31\u001b[0m | \u001b[1m  - wos5736\u001b[0m\n",
      "\u001b[32m17:21:31\u001b[0m | \u001b[1m  - sst1\u001b[0m\n",
      "\u001b[32m17:21:31\u001b[0m | \u001b[1m  - pang_movie\u001b[0m\n",
      "\u001b[32m17:21:31\u001b[0m | \u001b[1m  - movie_review\u001b[0m\n",
      "\u001b[32m17:21:31\u001b[0m | \u001b[1m  - vader_movie\u001b[0m\n",
      "\u001b[32m17:21:31\u001b[0m | \u001b[1m  - subj\u001b[0m\n",
      "\u001b[32m17:21:31\u001b[0m | \u001b[1m  - sst2\u001b[0m\n",
      "\u001b[32m17:21:31\u001b[0m | \u001b[1m  - yelp_reviews\u001b[0m\n",
      "\u001b[32m17:21:31\u001b[0m | \u001b[1m  - 20ng\u001b[0m\n",
      "\u001b[32m17:21:31\u001b[0m | \u001b[1m  - medline\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from src.utils.data_loader import load_hf_dataset, load_hf_dataset_as_dataframe, list_available_datasets\n",
    "\n",
    "# Listar datasets\n",
    "logger.info(\"Datasets dispon√≠veis:\")\n",
    "for dataset in list_available_datasets():\n",
    "    logger.info(f\"  - {dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m17:21:31\u001b[0m | \u001b[1mCarregando dataset: agnews\u001b[0m\n",
      "\u001b[32m17:21:31\u001b[0m | \u001b[1mCombinando splits: ['train', 'test']\u001b[0m\n",
      "\u001b[32m17:21:37\u001b[0m | \u001b[1m  ‚úì train: 510400 exemplos\u001b[0m\n",
      "\u001b[32m17:21:43\u001b[0m | \u001b[1m  ‚úì test: 127600 exemplos\u001b[0m\n",
      "\u001b[32m17:21:43\u001b[0m | \u001b[1mTotal combinado: 638000 exemplos\u001b[0m\n",
      "\u001b[32m17:21:43\u001b[0m | \u001b[1mCategorias extra√≠das automaticamente: [0, 1, 2, 3]\u001b[0m\n",
      "\u001b[32m17:21:44\u001b[0m | \u001b[1mColuna de texto: text\u001b[0m\n",
      "\u001b[32m17:21:44\u001b[0m | \u001b[1mGround truth carregado da coluna 'label'\u001b[0m\n",
      "\u001b[32m17:21:44\u001b[0m | \u001b[1mTextos: 638000\u001b[0m\n",
      "\u001b[32m17:21:44\u001b[0m | \u001b[1mCategorias: [0, 1, 2, 3]\u001b[0m\n",
      "\u001b[32m17:21:44\u001b[0m | \u001b[1mGround truth: Sim\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Carregar dataset\n",
    "dataset_name = \"agnews\"  # Ajuste conforme necess√°rio\n",
    "\n",
    "texts, categories, ground_truth = load_hf_dataset(dataset_name)\n",
    "\n",
    "logger.info(f\"Textos: {len(texts)}\")\n",
    "logger.info(f\"Categorias: {categories}\")\n",
    "logger.info(f\"Ground truth: {'Sim' if ground_truth else 'N√£o'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m17:21:44\u001b[0m | \u001b[1m\n",
      "Amostra dos textos:\u001b[0m\n",
      "\u001b[32m17:21:44\u001b[0m | \u001b[1m1. \"Irish Claim 2nd Title\",\"Notre Dame goalkeeper Erika Bohn seals the Irish's second NCAA championship...\u001b[0m\n",
      "\u001b[32m17:21:44\u001b[0m | \u001b[1m   Label: 1\u001b[0m\n",
      "\u001b[32m17:21:44\u001b[0m | \u001b[1m2. \"Court rules for BC in flap over exit fee\",\"Boston College cleared a major legal hurdle in its bid t...\u001b[0m\n",
      "\u001b[32m17:21:44\u001b[0m | \u001b[1m   Label: 1\u001b[0m\n",
      "\u001b[32m17:21:44\u001b[0m | \u001b[1m3. \"Scientific Method Man\",\"Gordon Rugg cracked the 400-year-old mystery of the Voynich manuscript. Nex...\u001b[0m\n",
      "\u001b[32m17:21:44\u001b[0m | \u001b[1m   Label: 3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Visualizar amostra\n",
    "logger.info(\"\\nAmostra dos textos:\")\n",
    "for i, text in enumerate(texts[:3]):\n",
    "    logger.info(f\"{i+1}. {text[:100]}...\")\n",
    "    if ground_truth:\n",
    "        logger.info(f\"   Label: {ground_truth[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m17:21:44\u001b[0m | \u001b[1mCarregando dataset: agnews\u001b[0m\n",
      "\u001b[32m17:21:44\u001b[0m | \u001b[1mCombinando splits: ['train', 'test']\u001b[0m\n",
      "\u001b[32m17:21:50\u001b[0m | \u001b[1m  ‚úì train: 510400 exemplos\u001b[0m\n",
      "\u001b[32m17:21:55\u001b[0m | \u001b[1m  ‚úì test: 127600 exemplos\u001b[0m\n",
      "\u001b[32m17:21:55\u001b[0m | \u001b[1mTotal combinado: 638000 exemplos\u001b[0m\n",
      "\u001b[32m17:21:55\u001b[0m | \u001b[1mCategorias extra√≠das automaticamente: [0, 1, 2, 3]\u001b[0m\n",
      "\u001b[32m17:21:56\u001b[0m | \u001b[1mColuna de texto: text\u001b[0m\n",
      "\u001b[32m17:21:56\u001b[0m | \u001b[1mGround truth carregado da coluna 'label'\u001b[0m\n",
      "\u001b[32m17:21:56\u001b[0m | \u001b[1mDataFrame criado com 638000 linhas\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "df, categories = load_hf_dataset_as_dataframe(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "iLocation based boolean indexing on an integer type is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_test = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\GitHub\\llm-annotation\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1192\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1190\u001b[39m maybe_callable = com.apply_if_callable(key, \u001b[38;5;28mself\u001b[39m.obj)\n\u001b[32m   1191\u001b[39m maybe_callable = \u001b[38;5;28mself\u001b[39m._check_deprecated_callable_usage(key, maybe_callable)\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\GitHub\\llm-annotation\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1739\u001b[39m, in \u001b[36m_iLocIndexer._getitem_axis\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1736\u001b[39m     key = np.asarray(key)\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m com.is_bool_indexer(key):\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1740\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getbool_axis(key, axis=axis)\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# a list of integers\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\GitHub\\llm-annotation\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1579\u001b[39m, in \u001b[36m_iLocIndexer._validate_key\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key.index, Index):\n\u001b[32m   1578\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key.index.inferred_type == \u001b[33m\"\u001b[39m\u001b[33minteger\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1579\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m   1580\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33miLocation based boolean \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1581\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mindexing on an integer type \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1582\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mis not available\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1583\u001b[39m         )\n\u001b[32m   1584\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1585\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33miLocation based boolean indexing cannot use \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1586\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33man indexable as a mask\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1587\u001b[39m     )\n\u001b[32m   1588\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[31mNotImplementedError\u001b[39m: iLocation based boolean indexing on an integer type is not available"
     ]
    }
   ],
   "source": [
    "df_test = df.iloc[df[\"label\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Configurar Modelos LLM\n",
    "\n",
    "### Op√ß√£o A: Usar apenas par√¢metros padr√£o (temp=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m17:11:32\u001b[0m | \u001b[1mCache LangChain ativado: ..\\..\\data\\.cache\\hf\\langchain_cache.db\u001b[0m\n",
      "\u001b[32m17:11:32\u001b[0m | \u001b[1mLLMAnnotator inicializado\u001b[0m\n",
      "\u001b[32m17:11:32\u001b[0m | \u001b[1mModelos: 5 | Categorias: 4\u001b[0m\n",
      "\u001b[32m17:11:32\u001b[0m | \u001b[32m\u001b[1m‚úì Annotator inicializado com 5 modelos\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from src.llm_annotation_system.annotation.llm_annotator import LLMAnnotator\n",
    "from src.experiments.base_experiment import DEFAULT_MODELS\n",
    "\n",
    "# Inicializar SEM alternative params\n",
    "annotator = LLMAnnotator(\n",
    "    models=DEFAULT_MODELS,\n",
    "    categories=categories,\n",
    "    api_keys=None,\n",
    "    use_langchain_cache=True,\n",
    "    use_alternative_params=False  # Apenas temp=0\n",
    ")\n",
    "\n",
    "logger.success(f\"‚úì Annotator inicializado com {len(annotator.models)} modelos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Op√ß√£o B: Usar alternative params (temp=0, 0.3, 0.5)\n",
    "\n",
    "**Aten√ß√£o**: Isso cria 9 modelos (3 base + 6 varia√ß√µes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomente para usar alternative params:\n",
    "\n",
    "# annotator = LLMAnnotator(\n",
    "#     models=models,\n",
    "#     categories=categories,\n",
    "#     api_keys=None,\n",
    "#     use_langchain_cache=True,\n",
    "#     use_alternative_params=True  # Expande para 9 modelos\n",
    "# )\n",
    "\n",
    "# logger.success(f\"‚úì Annotator com alternative params: {len(annotator.models)} modelos\")\n",
    "# logger.info(f\"  Modelos expandidos: {annotator.models}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Executar Anota√ß√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testar anota√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m17:12:30\u001b[0m | \u001b[33m\u001b[1m  Modelo: llama3-8b\u001b[0m\n",
      "\u001b[32m17:12:30\u001b[0m | \u001b[33m\u001b[1m  Texto: 191\u001b[0m\n",
      "\u001b[32m17:12:30\u001b[0m | \u001b[33m\u001b[1m  Repeti√ß√µes: 1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=ChatPromptTemplate(input_variables=['text'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='You are an expert data annotator with extensive experience in text classification tasks.\\n\\nYour task is to classify the following text into one of the predefined categories with high precision.\\n\\n**Instructions:**\\n1. Read the text carefully and understand its context\\n2. Consider the nuances and implicit meanings\\n3. Select the most appropriate category based on the content\\n4. Be consistent with your classification criteria\\n5. If the text is ambiguous, choose the most likely category based on dominant features\\n\\n**Available Categories:**\\n- 0: 0\\n- 1: 1\\n- 2: 2\\n- 3: 3\\n\\n**Text to classify:**\\n{text}\\n\\n**Important Guidelines:**\\n- Provide ONLY the category name as your response\\n- Do not include explanations unless specifically requested\\n- Be objective and avoid bias\\n- Consider edge cases carefully\\n- Maintain consistency across similar texts\\n\\n**Your classification (category name only):**'))]) middle=[ChatOllama(model='llama3:8b', num_predict=100, temperature=0.0)] last=StrOutputParser()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m17:12:35\u001b[0m | \u001b[31m\u001b[1mErro em llama3-8b rep 1: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/chat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000023044E26DD0>: Failed to establish a new connection: [WinError 10061] Nenhuma conex√£o p√¥de ser feita porque a m√°quina de destino as recusou ativamente'))\u001b[0m\n",
      "\u001b[32m17:12:35\u001b[0m | \u001b[32m\u001b[1m‚úì Anota√ß√£o completa\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ERROR']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Par√¢metros\n",
    "num_repetitions = 1\n",
    "text = texts[0]\n",
    "model = \"llama3-8b\"\n",
    "\n",
    "# Estimativa\n",
    "total_annotation = len(text) * num_repetitions\n",
    "logger.warning(f\"  Modelo: {model}\")\n",
    "logger.warning(f\"  Texto: {len(text)}\")\n",
    "logger.warning(f\"  Repeti√ß√µes: {num_repetitions}\")\n",
    "\n",
    "# Anotar\n",
    "annotations = annotator.annotate_single(\n",
    "    text=text,\n",
    "    model=model,\n",
    "    num_repetitions=num_repetitions\n",
    ")\n",
    "\n",
    "logger.success(\"‚úì Anota√ß√£o completa\")\n",
    "\n",
    "annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anotando dataset completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m17:11:36\u001b[0m | \u001b[1mTotal de anota√ß√µes: 1500\u001b[0m\n",
      "\u001b[32m17:11:36\u001b[0m | \u001b[33m\u001b[1m  Modelos: 5\u001b[0m\n",
      "\u001b[32m17:11:36\u001b[0m | \u001b[33m\u001b[1m  Textos: 100\u001b[0m\n",
      "\u001b[32m17:11:36\u001b[0m | \u001b[33m\u001b[1m  Repeti√ß√µes: 3\u001b[0m\n",
      "\u001b[32m17:11:36\u001b[0m | \u001b[1mIniciando anota√ß√£o\u001b[0m\n",
      "\u001b[32m17:11:36\u001b[0m | \u001b[1mTextos: 100 | Modelos: 5 | Repeti√ß√µes: 3\u001b[0m\n",
      "\u001b[32m17:11:36\u001b[0m | \u001b[1mTotal de anota√ß√µes: 1500\u001b[0m\n",
      "Anotando:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=ChatPromptTemplate(input_variables=['text'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='You are an expert data annotator with extensive experience in text classification tasks.\\n\\nYour task is to classify the following text into one of the predefined categories with high precision.\\n\\n**Instructions:**\\n1. Read the text carefully and understand its context\\n2. Consider the nuances and implicit meanings\\n3. Select the most appropriate category based on the content\\n4. Be consistent with your classification criteria\\n5. If the text is ambiguous, choose the most likely category based on dominant features\\n\\n**Available Categories:**\\n- 0: 0\\n- 1: 1\\n- 2: 2\\n- 3: 3\\n\\n**Text to classify:**\\n{text}\\n\\n**Important Guidelines:**\\n- Provide ONLY the category name as your response\\n- Do not include explanations unless specifically requested\\n- Be objective and avoid bias\\n- Consider edge cases carefully\\n- Maintain consistency across similar texts\\n\\n**Your classification (category name only):**'))]) middle=[ChatOllama(model='deepseek-r1:8b', num_predict=100, temperature=0.2)] last=StrOutputParser()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m17:11:40\u001b[0m | \u001b[31m\u001b[1mErro em deepseek-r1-8b rep 1: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/chat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000230421EFE90>: Failed to establish a new connection: [WinError 10061] Nenhuma conex√£o p√¥de ser feita porque a m√°quina de destino as recusou ativamente'))\u001b[0m\n",
      "\u001b[32m17:11:44\u001b[0m | \u001b[31m\u001b[1mErro em deepseek-r1-8b rep 2: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/chat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000230421F5490>: Failed to establish a new connection: [WinError 10061] Nenhuma conex√£o p√¥de ser feita porque a m√°quina de destino as recusou ativamente'))\u001b[0m\n",
      "\u001b[32m17:11:48\u001b[0m | \u001b[31m\u001b[1mErro em deepseek-r1-8b rep 3: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/chat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000230421F4390>: Failed to establish a new connection: [WinError 10061] Nenhuma conex√£o p√¥de ser feita porque a m√°quina de destino as recusou ativamente'))\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=ChatPromptTemplate(input_variables=['text'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='You are an expert data annotator with extensive experience in text classification tasks.\\n\\nYour task is to classify the following text into one of the predefined categories with high precision.\\n\\n**Instructions:**\\n1. Read the text carefully and understand its context\\n2. Consider the nuances and implicit meanings\\n3. Select the most appropriate category based on the content\\n4. Be consistent with your classification criteria\\n5. If the text is ambiguous, choose the most likely category based on dominant features\\n\\n**Available Categories:**\\n- 0: 0\\n- 1: 1\\n- 2: 2\\n- 3: 3\\n\\n**Text to classify:**\\n{text}\\n\\n**Important Guidelines:**\\n- Provide ONLY the category name as your response\\n- Do not include explanations unless specifically requested\\n- Be objective and avoid bias\\n- Consider edge cases carefully\\n- Maintain consistency across similar texts\\n\\n**Your classification (category name only):**'))]) middle=[ChatOllama(model='qwen2.5:7b', num_predict=100, temperature=0.2)] last=StrOutputParser()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m17:11:52\u001b[0m | \u001b[31m\u001b[1mErro em qwen2.5-7b rep 1: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/chat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000230421FEF50>: Failed to establish a new connection: [WinError 10061] Nenhuma conex√£o p√¥de ser feita porque a m√°quina de destino as recusou ativamente'))\u001b[0m\n",
      "\u001b[32m17:11:56\u001b[0m | \u001b[31m\u001b[1mErro em qwen2.5-7b rep 2: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/chat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000230421FFC90>: Failed to establish a new connection: [WinError 10061] Nenhuma conex√£o p√¥de ser feita porque a m√°quina de destino as recusou ativamente'))\u001b[0m\n",
      "\u001b[32m17:12:00\u001b[0m | \u001b[31m\u001b[1mErro em qwen2.5-7b rep 3: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/chat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000230421F5510>: Failed to establish a new connection: [WinError 10061] Nenhuma conex√£o p√¥de ser feita porque a m√°quina de destino as recusou ativamente'))\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=ChatPromptTemplate(input_variables=['text'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='You are an expert data annotator with extensive experience in text classification tasks.\\n\\nYour task is to classify the following text into one of the predefined categories with high precision.\\n\\n**Instructions:**\\n1. Read the text carefully and understand its context\\n2. Consider the nuances and implicit meanings\\n3. Select the most appropriate category based on the content\\n4. Be consistent with your classification criteria\\n5. If the text is ambiguous, choose the most likely category based on dominant features\\n\\n**Available Categories:**\\n- 0: 0\\n- 1: 1\\n- 2: 2\\n- 3: 3\\n\\n**Text to classify:**\\n{text}\\n\\n**Important Guidelines:**\\n- Provide ONLY the category name as your response\\n- Do not include explanations unless specifically requested\\n- Be objective and avoid bias\\n- Consider edge cases carefully\\n- Maintain consistency across similar texts\\n\\n**Your classification (category name only):**'))]) middle=[ChatOllama(model='gemma2:9b', num_predict=100, temperature=0.2)] last=StrOutputParser()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m17:12:04\u001b[0m | \u001b[31m\u001b[1mErro em gemma2-9b rep 1: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/chat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000230421EF250>: Failed to establish a new connection: [WinError 10061] Nenhuma conex√£o p√¥de ser feita porque a m√°quina de destino as recusou ativamente'))\u001b[0m\n",
      "\u001b[32m17:12:08\u001b[0m | \u001b[31m\u001b[1mErro em gemma2-9b rep 2: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/chat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002304220CA50>: Failed to establish a new connection: [WinError 10061] Nenhuma conex√£o p√¥de ser feita porque a m√°quina de destino as recusou ativamente'))\u001b[0m\n",
      "\u001b[32m17:12:12\u001b[0m | \u001b[31m\u001b[1mErro em gemma2-9b rep 3: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/chat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000230421F5650>: Failed to establish a new connection: [WinError 10061] Nenhuma conex√£o p√¥de ser feita porque a m√°quina de destino as recusou ativamente'))\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=ChatPromptTemplate(input_variables=['text'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='You are an expert data annotator with extensive experience in text classification tasks.\\n\\nYour task is to classify the following text into one of the predefined categories with high precision.\\n\\n**Instructions:**\\n1. Read the text carefully and understand its context\\n2. Consider the nuances and implicit meanings\\n3. Select the most appropriate category based on the content\\n4. Be consistent with your classification criteria\\n5. If the text is ambiguous, choose the most likely category based on dominant features\\n\\n**Available Categories:**\\n- 0: 0\\n- 1: 1\\n- 2: 2\\n- 3: 3\\n\\n**Text to classify:**\\n{text}\\n\\n**Important Guidelines:**\\n- Provide ONLY the category name as your response\\n- Do not include explanations unless specifically requested\\n- Be objective and avoid bias\\n- Consider edge cases carefully\\n- Maintain consistency across similar texts\\n\\n**Your classification (category name only):**'))]) middle=[ChatOllama(model='mistral:7b', num_predict=100, temperature=0.0)] last=StrOutputParser()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Anotando:   0%|          | 0/100 [00:40<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\GitHub\\llm-annotation\\.venv\\Lib\\site-packages\\urllib3\\util\\connection.py:73\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     72\u001b[39m     sock.bind(source_address)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m sock.connect(sa)\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [WinError 10061] Nenhuma conex√£o p√¥de ser feita porque a m√°quina de destino as recusou ativamente",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m logger.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Repeti√ß√µes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_repetitions\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Anotar\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m df_annotations = \u001b[43mannotator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mannotate_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_repetitions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_repetitions\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m logger.success(\u001b[33m\"\u001b[39m\u001b[33m‚úì Anota√ß√µes completas\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m display(df_annotations.head())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\GitHub\\llm-annotation\\src\\llm_annotation_system\\annotation\\llm_annotator.py:212\u001b[39m, in \u001b[36mLLMAnnotator.annotate_dataset\u001b[39m\u001b[34m(self, texts, num_repetitions, prompt_template, examples, save_intermediate)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;66;03m# Anotar com cada modelo\u001b[39;00m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.models:\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     annotations = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mannotate_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_repetitions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_repetitions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexamples\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# Salvar repeti√ß√µes\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m rep_idx, annotation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(annotations):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\GitHub\\llm-annotation\\src\\llm_annotation_system\\annotation\\llm_annotator.py:162\u001b[39m, in \u001b[36mLLMAnnotator.annotate_single\u001b[39m\u001b[34m(self, text, model, num_repetitions, prompt_template, examples, use_cache)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mannotate_single\u001b[39m(\n\u001b[32m    140\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    141\u001b[39m     text: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    146\u001b[39m     use_cache: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    147\u001b[39m ) -> List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    148\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[33;03m    Anota um texto √∫nico\u001b[39;00m\n\u001b[32m    150\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    160\u001b[39m \u001b[33;03m        Lista de classifica√ß√µes\u001b[39;00m\n\u001b[32m    161\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mannotation_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mannotate_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllms\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_repetitions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_repetitions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\GitHub\\llm-annotation\\src\\llm_annotation_system\\annotation\\annotation_engine.py:88\u001b[39m, in \u001b[36mAnnotationEngine.annotate_single\u001b[39m\u001b[34m(self, text, model, llm, num_repetitions, prompt_template, examples, use_cache)\u001b[39m\n\u001b[32m     86\u001b[39m     logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrep+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: cache hit\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m     \u001b[38;5;28mself\u001b[39m.cache.set(cache_key, response)\n\u001b[32m     90\u001b[39m     logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrep+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: cache miss\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\GitHub\\llm-annotation\\src\\llm_annotation_system\\annotation\\annotation_engine.py:167\u001b[39m, in \u001b[36mAnnotationEngine._invoke_chain\u001b[39m\u001b[34m(self, chain, text)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_invoke_chain\u001b[39m(\u001b[38;5;28mself\u001b[39m, chain: \u001b[38;5;28many\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    157\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    158\u001b[39m \u001b[33;03m    Invoca chain e retorna resposta\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    165\u001b[39m \u001b[33;03m        Resposta da LLM\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\GitHub\\llm-annotation\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2499\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config)\u001b[39m\n\u001b[32m   2497\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2498\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.steps):\n\u001b[32m-> \u001b[39m\u001b[32m2499\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2500\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2501\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[32m   2502\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2503\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2504\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2505\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2506\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   2507\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\GitHub\\llm-annotation\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:158\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    148\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    149\u001b[39m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[32m   (...)\u001b[39m\u001b[32m    153\u001b[39m     **kwargs: Any,\n\u001b[32m    154\u001b[39m ) -> BaseMessage:\n\u001b[32m    155\u001b[39m     config = ensure_config(config)\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    157\u001b[39m         ChatGeneration,\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    168\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\GitHub\\llm-annotation\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:560\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    553\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    554\u001b[39m     prompts: List[PromptValue],\n\u001b[32m   (...)\u001b[39m\u001b[32m    557\u001b[39m     **kwargs: Any,\n\u001b[32m    558\u001b[39m ) -> LLMResult:\n\u001b[32m    559\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\GitHub\\llm-annotation\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:421\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    419\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[32m    420\u001b[39m             run_managers[i].on_llm_error(e, response=LLMResult(generations=[]))\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    422\u001b[39m flattened_outputs = [\n\u001b[32m    423\u001b[39m     LLMResult(generations=[res.generations], llm_output=res.llm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[32m    424\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[32m    425\u001b[39m ]\n\u001b[32m    426\u001b[39m llm_output = \u001b[38;5;28mself\u001b[39m._combine_llm_outputs([res.llm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\GitHub\\llm-annotation\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:411\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[32m    409\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    410\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    417\u001b[39m         )\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    419\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\GitHub\\llm-annotation\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:632\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    631\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m632\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    635\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    636\u001b[39m         result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\GitHub\\llm-annotation\\.venv\\Lib\\site-packages\\langchain_community\\chat_models\\ollama.py:259\u001b[39m, in \u001b[36mChatOllama._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m    236\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    237\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    240\u001b[39m     **kwargs: Any,\n\u001b[32m    241\u001b[39m ) -> ChatResult:\n\u001b[32m    242\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call out to Ollama's generate endpoint.\u001b[39;00m\n\u001b[32m    243\u001b[39m \n\u001b[32m    244\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    256\u001b[39m \u001b[33;03m            ])\u001b[39;00m\n\u001b[32m    257\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m259\u001b[39m     final_chunk = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    266\u001b[39m     chat_generation = ChatGeneration(\n\u001b[32m    267\u001b[39m         message=AIMessage(content=final_chunk.text),\n\u001b[32m    268\u001b[39m         generation_info=final_chunk.generation_info,\n\u001b[32m    269\u001b[39m     )\n\u001b[32m    270\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(generations=[chat_generation])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\GitHub\\llm-annotation\\.venv\\Lib\\site-packages\\langchain_community\\chat_models\\ollama.py:190\u001b[39m, in \u001b[36mChatOllama._chat_stream_with_aggregation\u001b[39m\u001b[34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_stream_with_aggregation\u001b[39m(\n\u001b[32m    182\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    183\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    187\u001b[39m     **kwargs: Any,\n\u001b[32m    188\u001b[39m ) -> ChatGenerationChunk:\n\u001b[32m    189\u001b[39m     final_chunk: Optional[ChatGenerationChunk] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m            \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_chat_stream_response_to_chat_generation_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\GitHub\\llm-annotation\\.venv\\Lib\\site-packages\\langchain_community\\chat_models\\ollama.py:162\u001b[39m, in \u001b[36mChatOllama._create_chat_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_chat_stream\u001b[39m(\n\u001b[32m    153\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    154\u001b[39m     messages: List[BaseMessage],\n\u001b[32m    155\u001b[39m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    156\u001b[39m     **kwargs: Any,\n\u001b[32m    157\u001b[39m ) -> Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    158\u001b[39m     payload = {\n\u001b[32m    159\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    160\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._convert_messages_to_ollama_messages(messages),\n\u001b[32m    161\u001b[39m     }\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_url\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/api/chat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\GitHub\\llm-annotation\\.venv\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:231\u001b[39m, in \u001b[36m_OllamaCommon._create_stream\u001b[39m\u001b[34m(self, api_url, payload, stop, **kwargs)\u001b[39m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    225\u001b[39m     request_payload = {\n\u001b[32m    226\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: payload.get(\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    227\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mimages\u001b[39m\u001b[33m\"\u001b[39m: payload.get(\u001b[33m\"\u001b[39m\u001b[33mimages\u001b[39m\u001b[33m\"\u001b[39m, []),\n\u001b[32m    228\u001b[39m         **params,\n\u001b[32m    229\u001b[39m     }\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mContent-Type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mapplication/json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m response.encoding = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code != \u001b[32m200\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\GitHub\\llm-annotation\\.venv\\Lib\\site-packages\\requests\\api.py:115\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\GitHub\\llm-annotation\\.venv\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\GitHub\\llm-annotation\\.venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\GitHub\\llm-annotation\\.venv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\GitHub\\llm-annotation\\.venv\\Lib\\site-packages\\requests\\adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\GitHub\\llm-annotation\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\GitHub\\llm-annotation\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:493\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[32m    491\u001b[39m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[32m    505\u001b[39m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[32m    506\u001b[39m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBrokenPipeError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\GitHub\\llm-annotation\\.venv\\Lib\\site-packages\\urllib3\\connection.py:494\u001b[39m, in \u001b[36mHTTPConnection.request\u001b[39m\u001b[34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m header, value \u001b[38;5;129;01min\u001b[39;00m headers.items():\n\u001b[32m    493\u001b[39m     \u001b[38;5;28mself\u001b[39m.putheader(header, value)\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n\u001b[32m    497\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\http\\client.py:1298\u001b[39m, in \u001b[36mHTTPConnection.endheaders\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1296\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1297\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[32m-> \u001b[39m\u001b[32m1298\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\http\\client.py:1058\u001b[39m, in \u001b[36mHTTPConnection._send_output\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1056\u001b[39m msg = \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mself\u001b[39m._buffer)\n\u001b[32m   1057\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffer[:]\n\u001b[32m-> \u001b[39m\u001b[32m1058\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1060\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1061\u001b[39m \n\u001b[32m   1062\u001b[39m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n\u001b[32m   1063\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(message_body, \u001b[33m'\u001b[39m\u001b[33mread\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m   1064\u001b[39m         \u001b[38;5;66;03m# Let file-like take precedence over byte-like.  This\u001b[39;00m\n\u001b[32m   1065\u001b[39m         \u001b[38;5;66;03m# is needed to allow the current position of mmap'ed\u001b[39;00m\n\u001b[32m   1066\u001b[39m         \u001b[38;5;66;03m# files to be taken into account.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\http\\client.py:996\u001b[39m, in \u001b[36mHTTPConnection.send\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    994\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.sock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    995\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_open:\n\u001b[32m--> \u001b[39m\u001b[32m996\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    997\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    998\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m NotConnected()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\GitHub\\llm-annotation\\.venv\\Lib\\site-packages\\urllib3\\connection.py:325\u001b[39m, in \u001b[36mHTTPConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     \u001b[38;5;28mself\u001b[39m.sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tunnel_host:\n\u001b[32m    327\u001b[39m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mself\u001b[39m._has_connected_to_proxy = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\GitHub\\llm-annotation\\.venv\\Lib\\site-packages\\urllib3\\connection.py:198\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[32m    194\u001b[39m \n\u001b[32m    195\u001b[39m \u001b[33;03m:return: New socket connection.\u001b[39;00m\n\u001b[32m    196\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     sock = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m.host, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\GitHub\\llm-annotation\\.venv\\Lib\\site-packages\\urllib3\\util\\connection.py:81\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     79\u001b[39m         err = _\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m sock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m             \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\socket.py:499\u001b[39m, in \u001b[36msocket.close\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_real_close\u001b[39m(\u001b[38;5;28mself\u001b[39m, _ss=_socket.socket):\n\u001b[32m    496\u001b[39m     \u001b[38;5;66;03m# This function should not reference any globals. See issue #808164.\u001b[39;00m\n\u001b[32m    497\u001b[39m     _ss.close(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m499\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    500\u001b[39m     \u001b[38;5;66;03m# This function should not reference any globals. See issue #808164.\u001b[39;00m\n\u001b[32m    501\u001b[39m     \u001b[38;5;28mself\u001b[39m._closed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._io_refs <= \u001b[32m0\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Par√¢metros\n",
    "num_repetitions = 3\n",
    "\n",
    "# Estimativa\n",
    "total_annotations = len(texts) * len(annotator.models) * num_repetitions\n",
    "logger.info(f\"Total de anota√ß√µes: {total_annotations}\")\n",
    "logger.warning(f\"  Modelos: {len(annotator.models)}\")\n",
    "logger.warning(f\"  Textos: {len(texts)}\")\n",
    "logger.warning(f\"  Repeti√ß√µes: {num_repetitions}\")\n",
    "\n",
    "# Anotar\n",
    "df_annotations = annotator.annotate_dataset(\n",
    "    texts=texts,\n",
    "    num_repetitions=num_repetitions\n",
    ")\n",
    "\n",
    "logger.success(\"‚úì Anota√ß√µes completas\")\n",
    "display(df_annotations.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Calcular Consenso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular consenso\n",
    "df_with_consensus = annotator.calculate_consensus(df_annotations)\n",
    "\n",
    "# Estat√≠sticas\n",
    "logger.info(\"\\nüìä Estat√≠sticas de Consenso:\")\n",
    "logger.info(f\"  M√©dia: {df_with_consensus['consensus_score'].mean():.2%}\")\n",
    "logger.info(f\"  Mediana: {df_with_consensus['consensus_score'].median():.2%}\")\n",
    "logger.info(f\"  Desvio padr√£o: {df_with_consensus['consensus_score'].std():.2%}\")\n",
    "\n",
    "# Distribui√ß√£o por n√≠vel\n",
    "levels = df_with_consensus['consensus_level'].value_counts()\n",
    "logger.info(\"\\nDistribui√ß√£o por n√≠vel:\")\n",
    "for level, count in levels.items():\n",
    "    logger.info(f\"  {level}: {count} ({count/len(df_with_consensus):.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√µes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histograma\n",
    "axes[0].hist(df_with_consensus['consensus_score'], bins=20, edgecolor='black')\n",
    "axes[0].set_xlabel('Consensus Score')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribui√ß√£o de Scores de Consenso')\n",
    "\n",
    "# Barras por n√≠vel\n",
    "levels.plot(kind='bar', ax=axes[1], color=['green', 'orange', 'red'])\n",
    "axes[1].set_xlabel('N√≠vel de Consenso')\n",
    "axes[1].set_ylabel('Contagem')\n",
    "axes[1].set_title('Casos por N√≠vel de Consenso')\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ An√°lise de Alternative Params\n",
    "\n",
    "**Nota**: Esta se√ß√£o s√≥ funciona se `use_alternative_params=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar se alternative params foi usado\n",
    "if annotator.use_alternative_params:\n",
    "    logger.info(\"üìä Analisando impacto dos alternative params...\")\n",
    "    \n",
    "    # Agrupar por modelo base\n",
    "    for base_model in models:\n",
    "        # Encontrar varia√ß√µes deste modelo\n",
    "        variations = [m for m in annotator.models if m.startswith(base_model)]\n",
    "        \n",
    "        logger.info(f\"\\n{base_model}:\")\n",
    "        \n",
    "        for var in variations:\n",
    "            if f'{var}_consensus_score' in df_with_consensus.columns:\n",
    "                score = df_with_consensus[f'{var}_consensus_score'].mean()\n",
    "                logger.info(f\"  {var}: {score:.2%} consenso interno\")\n",
    "    \n",
    "    # Comparar temperaturas\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    consensus_cols = [col for col in df_with_consensus.columns \n",
    "                     if '_consensus_score' in col and '_alt' in col or \n",
    "                     (col.replace('_consensus_score', '') in models)]\n",
    "    \n",
    "    if consensus_cols:\n",
    "        means = [df_with_consensus[col].mean() for col in consensus_cols]\n",
    "        labels = [col.replace('_consensus_score', '') for col in consensus_cols]\n",
    "        \n",
    "        ax.bar(labels, means)\n",
    "        ax.set_ylabel('Consenso Interno M√©dio')\n",
    "        ax.set_title('Consenso por Varia√ß√£o de Par√¢metros')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "else:\n",
    "    logger.warning(\"Alternative params n√£o foi usado. Para an√°lise detalhada, reinicialize com use_alternative_params=True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ An√°lise Detalhada de Consenso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from consensus_analyzer_refactored import ConsensusAnalyzer\n",
    "\n",
    "# Inicializar analyzer\n",
    "analyzer = ConsensusAnalyzer(categories)\n",
    "\n",
    "# Colunas de consenso\n",
    "consensus_cols = [col for col in df_with_consensus.columns if '_consensus' in col and '_score' not in col]\n",
    "\n",
    "logger.info(f\"Analisando {len(consensus_cols)} anotadores\")\n",
    "\n",
    "# Gerar relat√≥rio\n",
    "report = analyzer.generate_consensus_report(\n",
    "    df=df_with_consensus,\n",
    "    annotator_cols=consensus_cols,\n",
    "    output_dir=\"./results\"\n",
    ")\n",
    "\n",
    "logger.success(\"‚úì Relat√≥rio gerado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©tricas\n",
    "logger.info(\"\\nüìä M√©tricas de Concord√¢ncia:\")\n",
    "logger.info(f\"  Fleiss' Kappa: {report['fleiss_kappa']:.3f} ({report['fleiss_interpretation']})\")\n",
    "\n",
    "# Interpreta√ß√£o\n",
    "kappa = report['fleiss_kappa']\n",
    "if kappa > 0.8:\n",
    "    logger.success(\"Concord√¢ncia excelente!\")\n",
    "elif kappa > 0.6:\n",
    "    logger.info(\"Concord√¢ncia boa\")\n",
    "elif kappa > 0.4:\n",
    "    logger.warning(\"Concord√¢ncia moderada\")\n",
    "else:\n",
    "    logger.warning(\"Concord√¢ncia fraca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de concord√¢ncia\n",
    "agreement_df = report['pairwise_agreement']\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(agreement_df, annot=True, fmt='.2f', cmap='YlGnBu', cbar_kws={'label': 'Agreement'})\n",
    "plt.title('Matriz de Concord√¢ncia Par a Par')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casos problem√°ticos\n",
    "problematic = report.get('problematic_cases')\n",
    "if problematic is not None and len(problematic) > 0:\n",
    "    logger.warning(f\"\\n‚ö†Ô∏è  {len(problematic)} casos problem√°ticos identificados\")\n",
    "    display(problematic.head())\n",
    "else:\n",
    "    logger.success(\"\\n‚úì Nenhum caso problem√°tico identificado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Valida√ß√£o com Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ground_truth:\n",
    "    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "    \n",
    "    # Adicionar ground truth\n",
    "    df_with_consensus['ground_truth'] = ground_truth\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(\n",
    "        df_with_consensus['ground_truth'],\n",
    "        df_with_consensus['most_common_annotation']\n",
    "    )\n",
    "    \n",
    "    logger.success(f\"\\nüéØ Accuracy: {accuracy:.2%}\")\n",
    "    \n",
    "    # Classification report\n",
    "    logger.info(\"\\nClassification Report:\")\n",
    "    print(classification_report(\n",
    "        df_with_consensus['ground_truth'],\n",
    "        df_with_consensus['most_common_annotation']\n",
    "    ))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(\n",
    "        df_with_consensus['ground_truth'],\n",
    "        df_with_consensus['most_common_annotation']\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=categories, yticklabels=categories)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix vs Ground Truth')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./results/confusion_vs_ground_truth.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    logger.info(\"\\n‚ö†Ô∏è  Ground truth n√£o dispon√≠vel - pulando valida√ß√£o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Exportar Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Criar diret√≥rio\n",
    "results_dir = Path('./results/final')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Salvar CSVs\n",
    "df_with_consensus.to_csv(results_dir / 'dataset_anotado_completo.csv', index=False)\n",
    "logger.info(f\"‚úì Salvos: {len(df_with_consensus)} registros\")\n",
    "\n",
    "# Alta confian√ßa\n",
    "high_conf = df_with_consensus[df_with_consensus['consensus_score'] >= 0.8]\n",
    "high_conf.to_csv(results_dir / 'alta_confianca.csv', index=False)\n",
    "logger.info(f\"‚úì Alta confian√ßa: {len(high_conf)} registros\")\n",
    "\n",
    "# Necessita revis√£o\n",
    "low_conf = df_with_consensus[df_with_consensus['consensus_score'] < 0.8]\n",
    "low_conf.to_csv(results_dir / 'necessita_revisao.csv', index=False)\n",
    "logger.info(f\"‚úì Necessita revis√£o: {len(low_conf)} registros\")\n",
    "\n",
    "# Sum√°rio JSON\n",
    "summary = {\n",
    "    'dataset': {\n",
    "        'name': dataset_name,\n",
    "        'total_texts': len(texts),\n",
    "        'categories': categories,\n",
    "        'has_ground_truth': ground_truth is not None\n",
    "    },\n",
    "    'config': {\n",
    "        'models': models,\n",
    "        'total_models': len(annotator.models),\n",
    "        'use_alternative_params': annotator.use_alternative_params,\n",
    "        'num_repetitions': num_repetitions,\n",
    "        'total_annotations': len(texts) * len(annotator.models) * num_repetitions\n",
    "    },\n",
    "    'results': {\n",
    "        'consensus_mean': float(df_with_consensus['consensus_score'].mean()),\n",
    "        'consensus_median': float(df_with_consensus['consensus_score'].median()),\n",
    "        'high_consensus': int((df_with_consensus['consensus_level'] == 'high').sum()),\n",
    "        'medium_consensus': int((df_with_consensus['consensus_level'] == 'medium').sum()),\n",
    "        'low_consensus': int((df_with_consensus['consensus_level'] == 'low').sum()),\n",
    "    },\n",
    "    'metrics': {\n",
    "        'fleiss_kappa': float(report['fleiss_kappa']),\n",
    "        'fleiss_interpretation': report['fleiss_interpretation']\n",
    "    }\n",
    "}\n",
    "\n",
    "if ground_truth:\n",
    "    summary['validation'] = {\n",
    "        'accuracy': float(accuracy)\n",
    "    }\n",
    "\n",
    "with open(results_dir / 'sumario_experimento.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "logger.success(\"\\n‚úì Resultados exportados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü Resumo Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.success(\"RESUMO DO EXPERIMENTO\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "logger.info(f\"\\nüìä Dataset: {dataset_name}\")\n",
    "logger.info(f\"  Textos: {len(texts)}\")\n",
    "logger.info(f\"  Categorias: {len(categories)}\")\n",
    "\n",
    "logger.info(f\"\\nü§ñ Configura√ß√£o:\")\n",
    "logger.info(f\"  Modelos base: {len(models)}\")\n",
    "logger.info(f\"  Total modelos: {len(annotator.models)}\")\n",
    "logger.info(f\"  Alternative params: {annotator.use_alternative_params}\")\n",
    "logger.info(f\"  Repeti√ß√µes: {num_repetitions}\")\n",
    "\n",
    "logger.info(f\"\\nüìà Consenso:\")\n",
    "logger.info(f\"  M√©dia: {df_with_consensus['consensus_score'].mean():.2%}\")\n",
    "logger.info(f\"  Fleiss' Kappa: {report['fleiss_kappa']:.3f} ({report['fleiss_interpretation']})\")\n",
    "\n",
    "if ground_truth:\n",
    "    logger.info(f\"\\nüéØ Valida√ß√£o:\")\n",
    "    logger.info(f\"  Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "logger.info(f\"\\nüìÅ Arquivos gerados em: {results_dir}/\")\n",
    "\n",
    "cache_stats = annotator.get_cache_stats()\n",
    "logger.info(f\"\\nüíæ Cache: {cache_stats['total_entries']} entradas\")\n",
    "\n",
    "logger.success(\"\\n‚úÖ An√°lise completa!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-annotation-py3.11 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
