{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ An√°lise de Consenso entre LLMs\n",
    "## Notebook Refatorado com Alternative Params\n",
    "\n",
    "Este notebook usa:\n",
    "- Componentes modulares\n",
    "- Logging com loguru\n",
    "- Integra√ß√£o com HuggingFace\n",
    "- **Alternative params** para testar varia√ß√µes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup e Configura√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "\n",
    "# Configurar logging\n",
    "logger.remove()\n",
    "logger.add(\n",
    "    sys.stdout,\n",
    "    format=\"<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>\",\n",
    "    level=\"INFO\"\n",
    ")\n",
    "\n",
    "# Paths\n",
    "project_root = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "sys.path.insert(0, str(project_root / 'src' / 'llm_annotation_system'))\n",
    "sys.path.insert(0, str(project_root / 'src' / 'config'))\n",
    "sys.path.insert(0, str(project_root / 'src' / 'utils'))\n",
    "\n",
    "logger.success(\"‚úì Setup completo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Carregar Dataset do HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import load_hf_dataset, list_available_datasets\n",
    "\n",
    "# Listar datasets\n",
    "logger.info(\"Datasets dispon√≠veis:\")\n",
    "for dataset in list_available_datasets():\n",
    "    logger.info(f\"  - {dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dataset\n",
    "dataset_name = \"agnews\"  # Ajuste conforme necess√°rio\n",
    "\n",
    "texts, categories, ground_truth = load_hf_dataset(dataset_name)\n",
    "\n",
    "logger.info(f\"Textos: {len(texts)}\")\n",
    "logger.info(f\"Categorias: {categories}\")\n",
    "logger.info(f\"Ground truth: {'Sim' if ground_truth else 'N√£o'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar amostra\n",
    "logger.info(\"\\nAmostra dos textos:\")\n",
    "for i, text in enumerate(texts[:3]):\n",
    "    logger.info(f\"{i+1}. {text[:100]}...\")\n",
    "    if ground_truth:\n",
    "        logger.info(f\"   Label: {ground_truth[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Configurar Modelos LLM\n",
    "\n",
    "### Op√ß√£o A: Usar apenas par√¢metros padr√£o (temp=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_annotator_refactored import LLMAnnotator\n",
    "\n",
    "# Modelos base\n",
    "models = [\n",
    "    \"llama3-8b\",\n",
    "    \"mistral-7b\",\n",
    "    \"qwen2-7b\",\n",
    "]\n",
    "\n",
    "# Inicializar SEM alternative params\n",
    "annotator = LLMAnnotator(\n",
    "    models=models,\n",
    "    categories=categories,\n",
    "    api_keys=None,\n",
    "    use_langchain_cache=True,\n",
    "    use_alternative_params=False  # Apenas temp=0\n",
    ")\n",
    "\n",
    "logger.success(f\"‚úì Annotator inicializado com {len(annotator.models)} modelos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Op√ß√£o B: Usar alternative params (temp=0, 0.3, 0.5)\n",
    "\n",
    "**Aten√ß√£o**: Isso cria 9 modelos (3 base + 6 varia√ß√µes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomente para usar alternative params:\n",
    "\n",
    "# annotator = LLMAnnotator(\n",
    "#     models=models,\n",
    "#     categories=categories,\n",
    "#     api_keys=None,\n",
    "#     use_langchain_cache=True,\n",
    "#     use_alternative_params=True  # Expande para 9 modelos\n",
    "# )\n",
    "\n",
    "# logger.success(f\"‚úì Annotator com alternative params: {len(annotator.models)} modelos\")\n",
    "# logger.info(f\"  Modelos expandidos: {annotator.models}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Executar Anota√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Par√¢metros\n",
    "num_repetitions = 3\n",
    "\n",
    "# Estimativa\n",
    "total_annotations = len(texts) * len(annotator.models) * num_repetitions\n",
    "logger.info(f\"Total de anota√ß√µes: {total_annotations}\")\n",
    "logger.warning(f\"  Modelos: {len(annotator.models)}\")\n",
    "logger.warning(f\"  Textos: {len(texts)}\")\n",
    "logger.warning(f\"  Repeti√ß√µes: {num_repetitions}\")\n",
    "\n",
    "# Anotar\n",
    "df_annotations = annotator.annotate_dataset(\n",
    "    texts=texts,\n",
    "    num_repetitions=num_repetitions\n",
    ")\n",
    "\n",
    "logger.success(\"‚úì Anota√ß√µes completas\")\n",
    "display(df_annotations.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Calcular Consenso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular consenso\n",
    "df_with_consensus = annotator.calculate_consensus(df_annotations)\n",
    "\n",
    "# Estat√≠sticas\n",
    "logger.info(\"\\nüìä Estat√≠sticas de Consenso:\")\n",
    "logger.info(f\"  M√©dia: {df_with_consensus['consensus_score'].mean():.2%}\")\n",
    "logger.info(f\"  Mediana: {df_with_consensus['consensus_score'].median():.2%}\")\n",
    "logger.info(f\"  Desvio padr√£o: {df_with_consensus['consensus_score'].std():.2%}\")\n",
    "\n",
    "# Distribui√ß√£o por n√≠vel\n",
    "levels = df_with_consensus['consensus_level'].value_counts()\n",
    "logger.info(\"\\nDistribui√ß√£o por n√≠vel:\")\n",
    "for level, count in levels.items():\n",
    "    logger.info(f\"  {level}: {count} ({count/len(df_with_consensus):.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√µes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histograma\n",
    "axes[0].hist(df_with_consensus['consensus_score'], bins=20, edgecolor='black')\n",
    "axes[0].set_xlabel('Consensus Score')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribui√ß√£o de Scores de Consenso')\n",
    "\n",
    "# Barras por n√≠vel\n",
    "levels.plot(kind='bar', ax=axes[1], color=['green', 'orange', 'red'])\n",
    "axes[1].set_xlabel('N√≠vel de Consenso')\n",
    "axes[1].set_ylabel('Contagem')\n",
    "axes[1].set_title('Casos por N√≠vel de Consenso')\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ An√°lise de Alternative Params\n",
    "\n",
    "**Nota**: Esta se√ß√£o s√≥ funciona se `use_alternative_params=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar se alternative params foi usado\n",
    "if annotator.use_alternative_params:\n",
    "    logger.info(\"üìä Analisando impacto dos alternative params...\")\n",
    "    \n",
    "    # Agrupar por modelo base\n",
    "    for base_model in models:\n",
    "        # Encontrar varia√ß√µes deste modelo\n",
    "        variations = [m for m in annotator.models if m.startswith(base_model)]\n",
    "        \n",
    "        logger.info(f\"\\n{base_model}:\")\n",
    "        \n",
    "        for var in variations:\n",
    "            if f'{var}_consensus_score' in df_with_consensus.columns:\n",
    "                score = df_with_consensus[f'{var}_consensus_score'].mean()\n",
    "                logger.info(f\"  {var}: {score:.2%} consenso interno\")\n",
    "    \n",
    "    # Comparar temperaturas\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    consensus_cols = [col for col in df_with_consensus.columns \n",
    "                     if '_consensus_score' in col and '_alt' in col or \n",
    "                     (col.replace('_consensus_score', '') in models)]\n",
    "    \n",
    "    if consensus_cols:\n",
    "        means = [df_with_consensus[col].mean() for col in consensus_cols]\n",
    "        labels = [col.replace('_consensus_score', '') for col in consensus_cols]\n",
    "        \n",
    "        ax.bar(labels, means)\n",
    "        ax.set_ylabel('Consenso Interno M√©dio')\n",
    "        ax.set_title('Consenso por Varia√ß√£o de Par√¢metros')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "else:\n",
    "    logger.warning(\"Alternative params n√£o foi usado. Para an√°lise detalhada, reinicialize com use_alternative_params=True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ An√°lise Detalhada de Consenso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from consensus_analyzer_refactored import ConsensusAnalyzer\n",
    "\n",
    "# Inicializar analyzer\n",
    "analyzer = ConsensusAnalyzer(categories)\n",
    "\n",
    "# Colunas de consenso\n",
    "consensus_cols = [col for col in df_with_consensus.columns if '_consensus' in col and '_score' not in col]\n",
    "\n",
    "logger.info(f\"Analisando {len(consensus_cols)} anotadores\")\n",
    "\n",
    "# Gerar relat√≥rio\n",
    "report = analyzer.generate_consensus_report(\n",
    "    df=df_with_consensus,\n",
    "    annotator_cols=consensus_cols,\n",
    "    output_dir=\"./results\"\n",
    ")\n",
    "\n",
    "logger.success(\"‚úì Relat√≥rio gerado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©tricas\n",
    "logger.info(\"\\nüìä M√©tricas de Concord√¢ncia:\")\n",
    "logger.info(f\"  Fleiss' Kappa: {report['fleiss_kappa']:.3f} ({report['fleiss_interpretation']})\")\n",
    "\n",
    "# Interpreta√ß√£o\n",
    "kappa = report['fleiss_kappa']\n",
    "if kappa > 0.8:\n",
    "    logger.success(\"Concord√¢ncia excelente!\")\n",
    "elif kappa > 0.6:\n",
    "    logger.info(\"Concord√¢ncia boa\")\n",
    "elif kappa > 0.4:\n",
    "    logger.warning(\"Concord√¢ncia moderada\")\n",
    "else:\n",
    "    logger.warning(\"Concord√¢ncia fraca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de concord√¢ncia\n",
    "agreement_df = report['pairwise_agreement']\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(agreement_df, annot=True, fmt='.2f', cmap='YlGnBu', cbar_kws={'label': 'Agreement'})\n",
    "plt.title('Matriz de Concord√¢ncia Par a Par')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casos problem√°ticos\n",
    "problematic = report.get('problematic_cases')\n",
    "if problematic is not None and len(problematic) > 0:\n",
    "    logger.warning(f\"\\n‚ö†Ô∏è  {len(problematic)} casos problem√°ticos identificados\")\n",
    "    display(problematic.head())\n",
    "else:\n",
    "    logger.success(\"\\n‚úì Nenhum caso problem√°tico identificado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Valida√ß√£o com Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ground_truth:\n",
    "    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "    \n",
    "    # Adicionar ground truth\n",
    "    df_with_consensus['ground_truth'] = ground_truth\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(\n",
    "        df_with_consensus['ground_truth'],\n",
    "        df_with_consensus['most_common_annotation']\n",
    "    )\n",
    "    \n",
    "    logger.success(f\"\\nüéØ Accuracy: {accuracy:.2%}\")\n",
    "    \n",
    "    # Classification report\n",
    "    logger.info(\"\\nClassification Report:\")\n",
    "    print(classification_report(\n",
    "        df_with_consensus['ground_truth'],\n",
    "        df_with_consensus['most_common_annotation']\n",
    "    ))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(\n",
    "        df_with_consensus['ground_truth'],\n",
    "        df_with_consensus['most_common_annotation']\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=categories, yticklabels=categories)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix vs Ground Truth')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./results/confusion_vs_ground_truth.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    logger.info(\"\\n‚ö†Ô∏è  Ground truth n√£o dispon√≠vel - pulando valida√ß√£o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Exportar Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Criar diret√≥rio\n",
    "results_dir = Path('./results/final')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Salvar CSVs\n",
    "df_with_consensus.to_csv(results_dir / 'dataset_anotado_completo.csv', index=False)\n",
    "logger.info(f\"‚úì Salvos: {len(df_with_consensus)} registros\")\n",
    "\n",
    "# Alta confian√ßa\n",
    "high_conf = df_with_consensus[df_with_consensus['consensus_score'] >= 0.8]\n",
    "high_conf.to_csv(results_dir / 'alta_confianca.csv', index=False)\n",
    "logger.info(f\"‚úì Alta confian√ßa: {len(high_conf)} registros\")\n",
    "\n",
    "# Necessita revis√£o\n",
    "low_conf = df_with_consensus[df_with_consensus['consensus_score'] < 0.8]\n",
    "low_conf.to_csv(results_dir / 'necessita_revisao.csv', index=False)\n",
    "logger.info(f\"‚úì Necessita revis√£o: {len(low_conf)} registros\")\n",
    "\n",
    "# Sum√°rio JSON\n",
    "summary = {\n",
    "    'dataset': {\n",
    "        'name': dataset_name,\n",
    "        'total_texts': len(texts),\n",
    "        'categories': categories,\n",
    "        'has_ground_truth': ground_truth is not None\n",
    "    },\n",
    "    'config': {\n",
    "        'models': models,\n",
    "        'total_models': len(annotator.models),\n",
    "        'use_alternative_params': annotator.use_alternative_params,\n",
    "        'num_repetitions': num_repetitions,\n",
    "        'total_annotations': len(texts) * len(annotator.models) * num_repetitions\n",
    "    },\n",
    "    'results': {\n",
    "        'consensus_mean': float(df_with_consensus['consensus_score'].mean()),\n",
    "        'consensus_median': float(df_with_consensus['consensus_score'].median()),\n",
    "        'high_consensus': int((df_with_consensus['consensus_level'] == 'high').sum()),\n",
    "        'medium_consensus': int((df_with_consensus['consensus_level'] == 'medium').sum()),\n",
    "        'low_consensus': int((df_with_consensus['consensus_level'] == 'low').sum()),\n",
    "    },\n",
    "    'metrics': {\n",
    "        'fleiss_kappa': float(report['fleiss_kappa']),\n",
    "        'fleiss_interpretation': report['fleiss_interpretation']\n",
    "    }\n",
    "}\n",
    "\n",
    "if ground_truth:\n",
    "    summary['validation'] = {\n",
    "        'accuracy': float(accuracy)\n",
    "    }\n",
    "\n",
    "with open(results_dir / 'sumario_experimento.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "logger.success(\"\\n‚úì Resultados exportados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü Resumo Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.success(\"RESUMO DO EXPERIMENTO\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "logger.info(f\"\\nüìä Dataset: {dataset_name}\")\n",
    "logger.info(f\"  Textos: {len(texts)}\")\n",
    "logger.info(f\"  Categorias: {len(categories)}\")\n",
    "\n",
    "logger.info(f\"\\nü§ñ Configura√ß√£o:\")\n",
    "logger.info(f\"  Modelos base: {len(models)}\")\n",
    "logger.info(f\"  Total modelos: {len(annotator.models)}\")\n",
    "logger.info(f\"  Alternative params: {annotator.use_alternative_params}\")\n",
    "logger.info(f\"  Repeti√ß√µes: {num_repetitions}\")\n",
    "\n",
    "logger.info(f\"\\nüìà Consenso:\")\n",
    "logger.info(f\"  M√©dia: {df_with_consensus['consensus_score'].mean():.2%}\")\n",
    "logger.info(f\"  Fleiss' Kappa: {report['fleiss_kappa']:.3f} ({report['fleiss_interpretation']})\")\n",
    "\n",
    "if ground_truth:\n",
    "    logger.info(f\"\\nüéØ Valida√ß√£o:\")\n",
    "    logger.info(f\"  Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "logger.info(f\"\\nüìÅ Arquivos gerados em: {results_dir}/\")\n",
    "\n",
    "cache_stats = annotator.get_cache_stats()\n",
    "logger.info(f\"\\nüíæ Cache: {cache_stats['total_entries']} entradas\")\n",
    "\n",
    "logger.success(\"\\n‚úÖ An√°lise completa!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
