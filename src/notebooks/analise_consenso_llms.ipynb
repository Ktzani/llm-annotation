{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Sistema de Anota√ß√£o Autom√°tica com M√∫ltiplas LLMs\n",
    "\n",
    "Este notebook demonstra o uso completo do sistema com datasets do HuggingFace.\n",
    "\n",
    "## üìã Conte√∫do\n",
    "\n",
    "1. Setup e Configura√ß√£o\n",
    "2. Carregar Dataset do HuggingFace\n",
    "3. Configurar Modelos LLM\n",
    "4. Executar Anota√ß√£o\n",
    "5. Calcular Consenso\n",
    "6. An√°lise Detalhada\n",
    "7. Visualiza√ß√µes\n",
    "8. Valida√ß√£o com Ground Truth\n",
    "9. Exportar Resultados\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup e Configura√ß√£o\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m project_root = PathLib.cwd().parent \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mnotebooks\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(PathLib.cwd()) \u001b[38;5;28;01melse\u001b[39;00m PathLib.cwd()\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Adicionar diret√≥rio de config ao path\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m config_path = PathLib(\u001b[34;43m__file__\u001b[39;49m).parent.parent / \u001b[33m'\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     12\u001b[39m sys.path.insert(\u001b[32m0\u001b[39m, \u001b[38;5;28mstr\u001b[39m(config_path))\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProject root: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproject_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# Configurar paths\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Adicionar diret√≥rios ao path\n",
    "project_root = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "sys.path.insert(0, str(project_root / 'src' / 'llm_annotation_system'))\n",
    "sys.path.insert(0, str(project_root / 'src' / 'config'))\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BASE_ANNOTATION_PROMPT' from 'config' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m sys.path.append(os.path.abspath(\u001b[33m\"\u001b[39m\u001b[33m../..\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Sistema de anota√ß√£o\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllm_annotation_system\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllm_annotator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMAnnotator\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllm_annotation_system\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconsensus_analyzer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConsensusAnalyzer\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvisualizer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConsensusVisualizer\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\GitHub\\llm-annotation\\src\\llm_annotation_system\\llm_annotator.py:37\u001b[39m\n\u001b[32m     34\u001b[39m config_path = PathLib(\u001b[34m__file__\u001b[39m).parent.parent / \u001b[33m'\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     35\u001b[39m sys.path.insert(\u001b[32m0\u001b[39m, \u001b[38;5;28mstr\u001b[39m(config_path))\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     38\u001b[39m     BASE_ANNOTATION_PROMPT,\n\u001b[32m     39\u001b[39m     FEW_SHOT_PROMPT,\n\u001b[32m     40\u001b[39m     COT_PROMPT,\n\u001b[32m     41\u001b[39m     LLM_CONFIGS,\n\u001b[32m     42\u001b[39m     EXPERIMENT_CONFIG,\n\u001b[32m     43\u001b[39m )\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mLLMAnnotator\u001b[39;00m:\n\u001b[32m     47\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[33;03m    Classe principal para realizar anota√ß√µes autom√°ticas usando m√∫ltiplas LLMs\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'BASE_ANNOTATION_PROMPT' from 'config' (unknown location)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "from pathlib import Path as PathLib\n",
    "\n",
    "# Sistema de anota√ß√£o\n",
    "from src.llm_annotation_system.llm_annotator import LLMAnnotator\n",
    "from src.llm_annotation_system.consensus_analyzer import ConsensusAnalyzer\n",
    "from src.utils.visualizer import ConsensusVisualizer\n",
    "\n",
    "# Datasets HuggingFace\n",
    "from src.utils.data_loader import (\n",
    "    load_hf_dataset,\n",
    "    load_hf_dataset_as_dataframe,\n",
    "    list_available_datasets,\n",
    "    discover_dataset_structure\n",
    ")\n",
    "\n",
    "# Configurar visualiza√ß√µes\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ Imports carregados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregar API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'project_root' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Carregar vari√°veis de ambiente\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m env_path = \u001b[43mproject_root\u001b[49m / \u001b[33m'\u001b[39m\u001b[33m.env\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      5\u001b[39m load_dotenv(env_path)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Configurar API keys\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'project_root' is not defined"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Carregar vari√°veis de ambiente\n",
    "env_path = project_root / '.env'\n",
    "load_dotenv(env_path)\n",
    "\n",
    "# Configurar API keys\n",
    "api_keys = {\n",
    "    \"openai\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "    \"anthropic\": os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "    \"google\": os.getenv(\"GOOGLE_API_KEY\"),\n",
    "}\n",
    "\n",
    "# Verificar se as keys est√£o configuradas\n",
    "for provider, key in api_keys.items():\n",
    "    if key:\n",
    "        print(f\"‚úÖ {provider}: Configurada\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  {provider}: N√ÉO configurada\")\n",
    "\n",
    "print(\"\\nüí° Dica: Configure suas API keys no arquivo .env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Carregar Dataset do HuggingFace\n",
    "\n",
    "### 2.1 Listar Datasets Dispon√≠veis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver datasets configurados\n",
    "print(\"üìã Datasets configurados:\")\n",
    "for dataset_name in list_available_datasets():\n",
    "    print(f\"   ‚Ä¢ {dataset_name}\")\n",
    "\n",
    "print(\"\\nüí° Configure novos datasets em: src/config/dataset_config.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Descobrir Estrutura de um Dataset (Opcional)\n",
    "\n",
    "Se voc√™ quiser explorar um novo dataset antes de configur√°-lo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomente e ajuste para seu dataset\n",
    "# discover_dataset_structure(\"waashk/seu-dataset\", num_examples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Carregar Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escolha seu dataset\n",
    "dataset_name = \"exemplo_com_labels\"  # AJUSTE para seu dataset\n",
    "\n",
    "print(f\"üì¶ Carregando dataset: {dataset_name}\\n\")\n",
    "\n",
    "# Carregar\n",
    "texts, categories, ground_truth = load_hf_dataset(dataset_name)\n",
    "\n",
    "# Informa√ß√µes\n",
    "print(f\"\\nüìä Informa√ß√µes do Dataset:\")\n",
    "print(f\"   ‚Ä¢ Total de textos: {len(texts)}\")\n",
    "print(f\"   ‚Ä¢ Categorias: {categories}\")\n",
    "print(f\"   ‚Ä¢ Ground truth dispon√≠vel: {'Sim ‚úÖ' if ground_truth else 'N√£o'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Visualizar Amostra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar primeiros textos\n",
    "print(\"üìù Primeiros 5 textos do dataset:\\n\")\n",
    "for i in range(min(5, len(texts))):\n",
    "    print(f\"Texto {i+1}:\")\n",
    "    print(f\"   {texts[i][:100]}...\")\n",
    "    if ground_truth:\n",
    "        print(f\"   Label: {ground_truth[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 An√°lise Explorat√≥ria (se houver ground truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ground_truth:\n",
    "    # Criar DataFrame\n",
    "    df_explore = pd.DataFrame({\n",
    "        'text': texts,\n",
    "        'label': ground_truth\n",
    "    })\n",
    "    \n",
    "    print(\"üìä Distribui√ß√£o de Classes:\\n\")\n",
    "    print(df_explore['label'].value_counts())\n",
    "    \n",
    "    # Visualizar\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    df_explore['label'].value_counts().plot(kind='bar')\n",
    "    plt.title('Distribui√ß√£o de Classes no Dataset')\n",
    "    plt.xlabel('Classe')\n",
    "    plt.ylabel('Quantidade')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Sem ground truth - pulando an√°lise explorat√≥ria\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Configurar Modelos LLM\n",
    "\n",
    "### 3.1 Selecionar Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos a usar\n",
    "models = [\n",
    "    \"gpt-4-turbo\",\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"claude-3-opus\",\n",
    "    \"claude-3-sonnet\",\n",
    "    \"gemini-pro\",\n",
    "]\n",
    "\n",
    "print(\"ü§ñ Modelos selecionados:\")\n",
    "for i, model in enumerate(models, 1):\n",
    "    print(f\"   {i}. {model}\")\n",
    "\n",
    "print(f\"\\nüí° Dica: Para economizar, comece com 3 modelos (ex: gpt-3.5, claude-sonnet, gemini)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Inicializar Anotador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar diret√≥rios\n",
    "cache_dir = project_root / \"cache\"\n",
    "results_dir = project_root / \"results\"\n",
    "\n",
    "# Criar diret√≥rios se n√£o existirem\n",
    "cache_dir.mkdir(exist_ok=True)\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"üîß Inicializando anotador...\\n\")\n",
    "\n",
    "# Inicializar\n",
    "annotator = LLMAnnotator(\n",
    "    models=models,\n",
    "    categories=categories,\n",
    "    api_keys=api_keys,\n",
    "    cache_dir=str(cache_dir),\n",
    "    results_dir=str(results_dir)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Anotador configurado!\")\n",
    "print(f\"   ‚Ä¢ Modelos: {len(models)}\")\n",
    "print(f\"   ‚Ä¢ Categorias: {len(categories)}\")\n",
    "print(f\"   ‚Ä¢ Cache: {cache_dir}\")\n",
    "print(f\"   ‚Ä¢ Resultados: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Executar Anota√ß√£o\n",
    "\n",
    "### 4.1 Configurar Par√¢metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Par√¢metros da anota√ß√£o\n",
    "num_repetitions = 3  # Cada LLM anota 3 vezes\n",
    "test_param_variations = False  # True para testar varia√ß√µes de par√¢metros\n",
    "\n",
    "# Estimativa de custos\n",
    "total_annotations = len(texts) * len(models) * num_repetitions\n",
    "estimated_cost = total_annotations * 0.002  # Estimativa aproximada\n",
    "\n",
    "print(\"‚öôÔ∏è  Configura√ß√£o da Anota√ß√£o:\")\n",
    "print(f\"   ‚Ä¢ Textos: {len(texts)}\")\n",
    "print(f\"   ‚Ä¢ Modelos: {len(models)}\")\n",
    "print(f\"   ‚Ä¢ Repeti√ß√µes por modelo: {num_repetitions}\")\n",
    "print(f\"   ‚Ä¢ Total de anota√ß√µes: {total_annotations}\")\n",
    "print(f\"   ‚Ä¢ Custo estimado: ${estimated_cost:.2f}\")\n",
    "print(f\"\\n‚ö†Ô∏è  Come√ßando anota√ß√£o...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Executar Anota√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anotar dataset\n",
    "df_annotations = annotator.annotate_dataset(\n",
    "    texts=texts,\n",
    "    num_repetitions=num_repetitions,\n",
    "    test_param_variations=test_param_variations,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Anota√ß√£o conclu√≠da!\")\n",
    "print(f\"   ‚Ä¢ Shape: {df_annotations.shape}\")\n",
    "print(f\"   ‚Ä¢ Colunas: {len(df_annotations.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Visualizar Anota√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar primeiras linhas\n",
    "print(\"üìã Primeiras anota√ß√µes:\\n\")\n",
    "display(df_annotations.head())\n",
    "\n",
    "# Informa√ß√µes das colunas\n",
    "annotation_cols = [col for col in df_annotations.columns if col.startswith(('gpt', 'claude', 'gemini'))]\n",
    "print(f\"\\nüìä Colunas de anota√ß√£o: {len(annotation_cols)}\")\n",
    "print(f\"   Exemplos: {annotation_cols[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Calcular Consenso\n",
    "\n",
    "### 5.1 Executar An√°lise de Consenso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üßÆ Calculando consenso...\\n\")\n",
    "\n",
    "# Calcular consenso\n",
    "df_with_consensus = annotator.calculate_consensus(df_annotations)\n",
    "\n",
    "print(\"‚úÖ Consenso calculado!\")\n",
    "print(f\"   ‚Ä¢ Shape: {df_with_consensus.shape}\")\n",
    "\n",
    "# Mostrar novas colunas\n",
    "consensus_cols = [col for col in df_with_consensus.columns if 'consensus' in col.lower()]\n",
    "print(f\"\\nüìä Colunas de consenso adicionadas: {len(consensus_cols)}\")\n",
    "print(f\"   {consensus_cols[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Estat√≠sticas de Consenso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estat√≠sticas gerais\n",
    "print(\"üìä Estat√≠sticas de Consenso:\\n\")\n",
    "print(f\"   ‚Ä¢ Consenso m√©dio: {df_with_consensus['consensus_score'].mean():.2%}\")\n",
    "print(f\"   ‚Ä¢ Consenso mediano: {df_with_consensus['consensus_score'].median():.2%}\")\n",
    "print(f\"   ‚Ä¢ Desvio padr√£o: {df_with_consensus['consensus_score'].std():.2%}\")\n",
    "\n",
    "# Por n√≠vel de consenso\n",
    "high_consensus = (df_with_consensus['consensus_score'] >= 0.8).sum()\n",
    "medium_consensus = ((df_with_consensus['consensus_score'] >= 0.6) & \n",
    "                    (df_with_consensus['consensus_score'] < 0.8)).sum()\n",
    "low_consensus = (df_with_consensus['consensus_score'] < 0.6).sum()\n",
    "\n",
    "print(f\"\\nüìà Distribui√ß√£o por N√≠vel:\")\n",
    "print(f\"   ‚Ä¢ Alto consenso (‚â•80%): {high_consensus} ({high_consensus/len(df_with_consensus):.1%})\")\n",
    "print(f\"   ‚Ä¢ M√©dio consenso (60-80%): {medium_consensus} ({medium_consensus/len(df_with_consensus):.1%})\")\n",
    "print(f\"   ‚Ä¢ Baixo consenso (<60%): {low_consensus} ({low_consensus/len(df_with_consensus):.1%})\")\n",
    "\n",
    "# Casos problem√°ticos\n",
    "problematic = df_with_consensus['is_problematic'].sum()\n",
    "print(f\"\\n‚ö†Ô∏è  Casos problem√°ticos: {problematic} ({problematic/len(df_with_consensus):.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Visualizar Distribui√ß√£o de Consenso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograma de consenso\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df_with_consensus['consensus_score'], bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(0.8, color='green', linestyle='--', label='Alto consenso (80%)')\n",
    "plt.axvline(0.6, color='orange', linestyle='--', label='M√©dio consenso (60%)')\n",
    "plt.xlabel('Score de Consenso')\n",
    "plt.ylabel('Frequ√™ncia')\n",
    "plt.title('Distribui√ß√£o de Scores de Consenso')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "levels = ['Alto\\n(‚â•80%)', 'M√©dio\\n(60-80%)', 'Baixo\\n(<60%)']\n",
    "counts = [high_consensus, medium_consensus, low_consensus]\n",
    "colors = ['green', 'orange', 'red']\n",
    "plt.bar(levels, counts, color=colors, alpha=0.7)\n",
    "plt.ylabel('Quantidade')\n",
    "plt.title('Inst√¢ncias por N√≠vel de Consenso')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. An√°lise Detalhada\n",
    "\n",
    "### 6.1 Inicializar Analisador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî¨ Inicializando analisador de consenso...\\n\")\n",
    "\n",
    "# Criar analisador\n",
    "analyzer = ConsensusAnalyzer(categories=categories)\n",
    "\n",
    "# Identificar colunas de consenso\n",
    "consensus_annotation_cols = [col for col in df_with_consensus.columns \n",
    "                              if '_consensus' in col and '_score' not in col]\n",
    "\n",
    "print(f\"‚úÖ Analisador configurado\")\n",
    "print(f\"   ‚Ä¢ Categorias: {len(categories)}\")\n",
    "print(f\"   ‚Ä¢ Colunas de consenso: {len(consensus_annotation_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Gerar Relat√≥rio Completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Gerando relat√≥rio de consenso...\\n\")\n",
    "\n",
    "# Gerar relat√≥rio\n",
    "report = analyzer.generate_consensus_report(\n",
    "    df=df_with_consensus,\n",
    "    annotator_cols=consensus_annotation_cols,\n",
    "    output_dir=str(results_dir)\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Relat√≥rio gerado!\")\n",
    "print(f\"\\nüìã Conte√∫do do relat√≥rio:\")\n",
    "for key in report.keys():\n",
    "    print(f\"   ‚Ä¢ {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 M√©tricas de Dist√¢ncia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar m√©tricas de dist√¢ncia\n",
    "print(\"üìè M√©tricas de Dist√¢ncia e Concord√¢ncia:\\n\")\n",
    "\n",
    "distance_metrics = report.get('distance_metrics', {})\n",
    "for metric, value in distance_metrics.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"   ‚Ä¢ {metric}: {value:.4f}\")\n",
    "\n",
    "# Interpreta√ß√£o\n",
    "if 'mean_cohen_kappa' in distance_metrics:\n",
    "    kappa = distance_metrics['mean_cohen_kappa']\n",
    "    print(f\"\\nüìä Interpreta√ß√£o Cohen's Kappa ({kappa:.4f}):\")\n",
    "    if kappa > 0.8:\n",
    "        print(\"   ‚úÖ Excelente concord√¢ncia!\")\n",
    "    elif kappa > 0.6:\n",
    "        print(\"   ‚úÖ Boa concord√¢ncia\")\n",
    "    elif kappa > 0.4:\n",
    "        print(\"   ‚ö†Ô∏è  Concord√¢ncia moderada\")\n",
    "    else:\n",
    "        print(\"   ‚ùå Concord√¢ncia fraca\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Matriz de Concord√¢ncia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concord√¢ncia par a par\n",
    "if 'pairwise_agreement' in report:\n",
    "    print(\"ü§ù Concord√¢ncia Par a Par entre Modelos:\\n\")\n",
    "    \n",
    "    pairwise_agreement = report['pairwise_agreement']\n",
    "    display(pairwise_agreement)\n",
    "    \n",
    "    # Estat√≠sticas\n",
    "    print(f\"\\nüìä Estat√≠sticas:\")\n",
    "    print(f\"   ‚Ä¢ Concord√¢ncia m√©dia: {pairwise_agreement.values.mean():.2%}\")\n",
    "    print(f\"   ‚Ä¢ Concord√¢ncia m√≠nima: {pairwise_agreement.values.min():.2%}\")\n",
    "    print(f\"   ‚Ä¢ Concord√¢ncia m√°xima: {pairwise_agreement.values.max():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Casos Problem√°ticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar casos problem√°ticos\n",
    "problematic_cases = df_with_consensus[df_with_consensus['is_problematic']]\n",
    "\n",
    "print(f\"‚ö†Ô∏è  Casos Problem√°ticos: {len(problematic_cases)}\\n\")\n",
    "\n",
    "if len(problematic_cases) > 0:\n",
    "    # Mostrar alguns exemplos\n",
    "    print(\"üìù Exemplos de casos problem√°ticos:\\n\")\n",
    "    for idx in problematic_cases.head(3).index:\n",
    "        row = df_with_consensus.loc[idx]\n",
    "        print(f\"Texto {idx}:\")\n",
    "        print(f\"   {row['text'][:100]}...\")\n",
    "        print(f\"   Consenso: {row['consensus_score']:.0%}\")\n",
    "        print(f\"   Anota√ß√£o mais comum: {row['most_common_annotation']}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"‚úÖ Nenhum caso problem√°tico identificado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Visualiza√ß√µes\n",
    "\n",
    "### 7.1 Inicializar Visualizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar diret√≥rio de figuras\n",
    "figures_dir = results_dir / 'figures'\n",
    "figures_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"üìä Inicializando visualizador...\\n\")\n",
    "\n",
    "# Criar visualizador\n",
    "visualizer = ConsensusVisualizer(output_dir=str(figures_dir))\n",
    "\n",
    "print(f\"‚úÖ Visualizador configurado\")\n",
    "print(f\"   ‚Ä¢ Output: {figures_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Heatmap de Concord√¢ncia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar heatmap\n",
    "if 'pairwise_agreement' in report:\n",
    "    print(\"üî• Gerando heatmap de concord√¢ncia...\\n\")\n",
    "    \n",
    "    visualizer.plot_agreement_heatmap(\n",
    "        report['pairwise_agreement'],\n",
    "        title=\"Concord√¢ncia entre Modelos LLM\"\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Heatmap salvo em: {figures_dir}/agreement_heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Distribui√ß√£o de Consenso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Gerando gr√°fico de distribui√ß√£o de consenso...\\n\")\n",
    "\n",
    "visualizer.plot_consensus_distribution(df_with_consensus)\n",
    "\n",
    "print(f\"‚úÖ Gr√°fico salvo em: {figures_dir}/consensus_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Matriz de Confus√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confus√£o (se dispon√≠vel)\n",
    "if 'disagreement_patterns' in report and 'confusion_matrix' in report['disagreement_patterns']:\n",
    "    print(\"üéØ Gerando matriz de confus√£o...\\n\")\n",
    "    \n",
    "    visualizer.plot_confusion_matrix(\n",
    "        report['disagreement_patterns']['confusion_matrix']\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Matriz salva em: {figures_dir}/confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Compara√ß√£o de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ü§ñ Gerando compara√ß√£o de modelos...\\n\")\n",
    "\n",
    "visualizer.plot_model_comparison(\n",
    "    df_with_consensus,\n",
    "    models=models\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Compara√ß√£o salva em: {figures_dir}/model_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 Dashboard Interativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üåê Gerando dashboard interativo...\\n\")\n",
    "\n",
    "visualizer.create_interactive_dashboard(\n",
    "    df_with_consensus,\n",
    "    report\n",
    ")\n",
    "\n",
    "dashboard_path = figures_dir / 'interactive_dashboard.html'\n",
    "print(f\"‚úÖ Dashboard salvo em: {dashboard_path}\")\n",
    "print(f\"\\nüí° Abra no navegador para explorar: {dashboard_path.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Valida√ß√£o com Ground Truth\n",
    "\n",
    "Se o dataset tem labels, podemos validar a qualidade das anota√ß√µes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ground_truth:\n",
    "    print(\"‚úÖ Ground truth dispon√≠vel - validando anota√ß√µes...\\n\")\n",
    "    \n",
    "    # Adicionar ground truth ao DataFrame\n",
    "    df_with_consensus['ground_truth'] = ground_truth\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(\n",
    "        df_with_consensus['ground_truth'],\n",
    "        df_with_consensus['most_common_annotation']\n",
    "    )\n",
    "    \n",
    "    print(f\"üìä VALIDA√á√ÉO COM GROUND TRUTH\\n\")\n",
    "    print(f\"   ‚Ä¢ Accuracy geral: {accuracy:.2%}\\n\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"üìã Relat√≥rio de Classifica√ß√£o:\\n\")\n",
    "    print(classification_report(\n",
    "        df_with_consensus['ground_truth'],\n",
    "        df_with_consensus['most_common_annotation'],\n",
    "        target_names=categories\n",
    "    ))\n",
    "    \n",
    "    # Matriz de confus√£o\n",
    "    cm = confusion_matrix(\n",
    "        df_with_consensus['ground_truth'],\n",
    "        df_with_consensus['most_common_annotation']\n",
    "    )\n",
    "    \n",
    "    # Visualizar matriz de confus√£o\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=categories, yticklabels=categories)\n",
    "    plt.title('Matriz de Confus√£o: Consenso vs Ground Truth')\n",
    "    plt.ylabel('Ground Truth')\n",
    "    plt.xlabel('Consenso (Anota√ß√£o Autom√°tica)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figures_dir / 'confusion_vs_ground_truth.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Matriz salva em: {figures_dir}/confusion_vs_ground_truth.png\")\n",
    "    \n",
    "    # An√°lise por n√≠vel de consenso\n",
    "    print(\"\\nüìä Accuracy por N√≠vel de Consenso:\\n\")\n",
    "    \n",
    "    for threshold, name in [(0.8, 'Alto'), (0.6, 'M√©dio'), (0.0, 'Baixo')]:\n",
    "        mask = df_with_consensus['consensus_score'] >= threshold\n",
    "        if mask.sum() > 0:\n",
    "            acc = accuracy_score(\n",
    "                df_with_consensus[mask]['ground_truth'],\n",
    "                df_with_consensus[mask]['most_common_annotation']\n",
    "            )\n",
    "            print(f\"   ‚Ä¢ {name} (‚â•{threshold:.0%}): {acc:.2%} ({mask.sum()} inst√¢ncias)\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Ground truth n√£o dispon√≠vel - pulando valida√ß√£o\")\n",
    "    print(\"\\nüí° Dica: Use um dataset com labels para validar a qualidade!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Exportar Resultados\n",
    "\n",
    "### 9.1 Salvar Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üíæ Salvando resultados...\\n\")\n",
    "\n",
    "# Criar diret√≥rio final\n",
    "final_dir = results_dir / 'final'\n",
    "final_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 1. Dataset completo\n",
    "complete_path = final_dir / 'dataset_anotado_completo.csv'\n",
    "df_with_consensus.to_csv(complete_path, index=False, encoding='utf-8')\n",
    "print(f\"‚úÖ Dataset completo: {complete_path}\")\n",
    "\n",
    "# 2. Alta confian√ßa (‚â•80%)\n",
    "high_conf = df_with_consensus[df_with_consensus['consensus_score'] >= 0.8]\n",
    "high_conf_path = final_dir / 'alta_confianca.csv'\n",
    "high_conf.to_csv(high_conf_path, index=False, encoding='utf-8')\n",
    "print(f\"‚úÖ Alta confian√ßa ({len(high_conf)} inst√¢ncias): {high_conf_path}\")\n",
    "\n",
    "# 3. Necessita revis√£o (<80%)\n",
    "needs_review = df_with_consensus[df_with_consensus['consensus_score'] < 0.8]\n",
    "review_path = final_dir / 'necessita_revisao.csv'\n",
    "needs_review.to_csv(review_path, index=False, encoding='utf-8')\n",
    "print(f\"‚úÖ Necessita revis√£o ({len(needs_review)} inst√¢ncias): {review_path}\")\n",
    "\n",
    "# 4. Casos problem√°ticos\n",
    "if len(problematic_cases) > 0:\n",
    "    problematic_path = final_dir / 'casos_problematicos.csv'\n",
    "    problematic_cases.to_csv(problematic_path, index=False, encoding='utf-8')\n",
    "    print(f\"‚úÖ Casos problem√°ticos ({len(problematic_cases)} inst√¢ncias): {problematic_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Sum√°rio em JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Criar sum√°rio\n",
    "summary = {\n",
    "    'dataset': {\n",
    "        'name': dataset_name,\n",
    "        'total_instances': len(texts),\n",
    "        'categories': categories,\n",
    "        'has_ground_truth': ground_truth is not None,\n",
    "    },\n",
    "    'configuration': {\n",
    "        'models': models,\n",
    "        'num_models': len(models),\n",
    "        'num_repetitions': num_repetitions,\n",
    "        'total_annotations': total_annotations,\n",
    "    },\n",
    "    'results': {\n",
    "        'consensus': {\n",
    "            'mean': float(df_with_consensus['consensus_score'].mean()),\n",
    "            'median': float(df_with_consensus['consensus_score'].median()),\n",
    "            'std': float(df_with_consensus['consensus_score'].std()),\n",
    "        },\n",
    "        'distribution': {\n",
    "            'high_consensus': int(high_consensus),\n",
    "            'medium_consensus': int(medium_consensus),\n",
    "            'low_consensus': int(low_consensus),\n",
    "        },\n",
    "        'problematic_cases': int(problematic),\n",
    "    },\n",
    "    'metrics': distance_metrics if 'distance_metrics' in locals() else {},\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "}\n",
    "\n",
    "# Adicionar valida√ß√£o se dispon√≠vel\n",
    "if ground_truth:\n",
    "    summary['validation'] = {\n",
    "        'accuracy': float(accuracy),\n",
    "    }\n",
    "\n",
    "# Salvar\n",
    "summary_path = final_dir / 'sumario_experimento.json'\n",
    "with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Sum√°rio salvo: {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Resumo Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" \"*25 + \"RESUMO FINAL\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(f\"üìä DATASET\")\n",
    "print(f\"   ‚Ä¢ Nome: {dataset_name}\")\n",
    "print(f\"   ‚Ä¢ Total de textos: {len(texts)}\")\n",
    "print(f\"   ‚Ä¢ Categorias: {len(categories)}\")\n",
    "\n",
    "print(f\"\\nü§ñ CONFIGURA√á√ÉO\")\n",
    "print(f\"   ‚Ä¢ Modelos: {len(models)}\")\n",
    "print(f\"   ‚Ä¢ Repeti√ß√µes: {num_repetitions}\")\n",
    "print(f\"   ‚Ä¢ Total de anota√ß√µes: {total_annotations}\")\n",
    "\n",
    "print(f\"\\nüéØ CONSENSO\")\n",
    "print(f\"   ‚Ä¢ Consenso m√©dio: {df_with_consensus['consensus_score'].mean():.2%}\")\n",
    "print(f\"   ‚Ä¢ Alto consenso (‚â•80%): {high_consensus} ({high_consensus/len(df_with_consensus):.1%})\")\n",
    "print(f\"   ‚Ä¢ Casos problem√°ticos: {problematic} ({problematic/len(df_with_consensus):.1%})\")\n",
    "\n",
    "if 'mean_cohen_kappa' in distance_metrics:\n",
    "    print(f\"\\nüìè M√âTRICAS\")\n",
    "    print(f\"   ‚Ä¢ Cohen's Kappa: {distance_metrics['mean_cohen_kappa']:.4f}\")\n",
    "    if 'fleiss_kappa' in distance_metrics:\n",
    "        print(f\"   ‚Ä¢ Fleiss' Kappa: {distance_metrics['fleiss_kappa']:.4f}\")\n",
    "\n",
    "if ground_truth:\n",
    "    print(f\"\\n‚úÖ VALIDA√á√ÉO\")\n",
    "    print(f\"   ‚Ä¢ Accuracy vs Ground Truth: {accuracy:.2%}\")\n",
    "\n",
    "print(f\"\\nüìÅ ARQUIVOS GERADOS\")\n",
    "print(f\"   ‚Ä¢ Dataset completo: {complete_path.name}\")\n",
    "print(f\"   ‚Ä¢ Alta confian√ßa: {high_conf_path.name}\")\n",
    "print(f\"   ‚Ä¢ Necessita revis√£o: {review_path.name}\")\n",
    "print(f\"   ‚Ä¢ Dashboard interativo: interactive_dashboard.html\")\n",
    "print(f\"   ‚Ä¢ Sum√°rio JSON: {summary_path.name}\")\n",
    "\n",
    "print(f\"\\nüìÇ DIRET√ìRIOS\")\n",
    "print(f\"   ‚Ä¢ Resultados: {results_dir}\")\n",
    "print(f\"   ‚Ä¢ Figuras: {figures_dir}\")\n",
    "print(f\"   ‚Ä¢ Final: {final_dir}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ AN√ÅLISE COMPLETA!\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"üéâ Pr√≥ximos passos:\")\n",
    "print(\"   1. Abra o dashboard interativo no navegador\")\n",
    "print(\"   2. Analise casos de baixo consenso\")\n",
    "print(\"   3. Revise casos problem√°ticos\")\n",
    "if ground_truth:\n",
    "    print(\"   4. Analise erros vs ground truth\")\n",
    "print(\"   5. Documente seus achados\")\n",
    "print(\"   6. Prepare apresenta√ß√£o para orientador\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Conclus√µes e Pr√≥ximos Passos\n",
    "\n",
    "### An√°lise dos Resultados\n",
    "\n",
    "Analise os resultados considerando:\n",
    "\n",
    "1. **Taxa de consenso alto**: Quantos % das inst√¢ncias t√™m consenso ‚â•80%?\n",
    "2. **Cohen's Kappa**: A concord√¢ncia entre modelos √© boa (>0.6)?\n",
    "3. **Accuracy** (se tiver ground truth): As anota√ß√µes est√£o corretas?\n",
    "4. **Casos problem√°ticos**: Por que esses casos s√£o dif√≠ceis?\n",
    "\n",
    "### Quest√µes de Pesquisa\n",
    "\n",
    "1. Qual threshold de consenso devemos usar?\n",
    "2. Few-shot learning melhoraria os resultados?\n",
    "3. Quais modelos s√£o mais consistentes?\n",
    "4. Vale a pena o custo comparado com anota√ß√£o humana?\n",
    "\n",
    "### Melhorias Poss√≠veis\n",
    "\n",
    "1. **Prompts**: Testar few-shot ou Chain-of-Thought\n",
    "2. **Modelos**: Adicionar/remover modelos baseado em performance\n",
    "3. **Par√¢metros**: Testar diferentes temperaturas\n",
    "4. **Estrat√©gias**: Otimizar resolu√ß√£o de conflitos\n",
    "\n",
    "---\n",
    "\n",
    "**Boa sorte com sua pesquisa!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-annotation-py3.11 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
