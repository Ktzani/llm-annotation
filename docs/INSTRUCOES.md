# üìù Instru√ß√µes Espec√≠ficas para Gabriel Catizani

## üéØ O Que Voc√™ Tem Agora

Implementei um **sistema completo e profissional** para sua pesquisa em anota√ß√£o autom√°tica com LLMs. O c√≥digo est√° bem estruturado, documentado e pronto para apresentar ao seu orientador.

---

## üì¶ Arquivos Entregues

### C√≥digo Principal (4 m√≥dulos)

1. **config.py** (6.2 KB)
   - Prompts otimizados com prompt engineering
   - Configura√ß√µes de todos os modelos LLM
   - Par√¢metros do experimento
   - Estrat√©gias de resolu√ß√£o de conflitos

2. **llm_annotator.py** (17 KB)
   - Classe principal LLMAnnotator
   - Gerencia m√∫ltiplas LLMs simultaneamente
   - Sistema de cache para economizar API calls
   - Suporte para diferentes prompts e par√¢metros

3. **consensus_analyzer.py** (16 KB)
   - Classe ConsensusAnalyzer
   - Calcula todas as m√©tricas (Cohen's Kappa, Fleiss, etc.)
   - Identifica inst√¢ncias problem√°ticas
   - Gera relat√≥rio completo

4. **visualizer.py** (18 KB)
   - Classe ConsensusVisualizer
   - Gera todos os gr√°ficos
   - Dashboard interativo com Plotly
   - Exporta em m√∫ltiplos formatos

### Notebooks e Scripts

5. **analise_consenso_llms.ipynb** (27 KB) ‚≠ê **PRINCIPAL**
   - Notebook completo com an√°lise passo a passo
   - Explica√ß√µes detalhadas
   - Visualiza√ß√µes inline
   - Interpreta√ß√£o de resultados
   - **Use este para apresentar ao orientador**

6. **exemplo_uso.py** (4.5 KB)
   - Script de exemplo pronto para executar
   - Demonstra uso completo do sistema

### Documenta√ß√£o

7. **README.md** (6 KB)
   - Documenta√ß√£o completa do projeto
   - Guia de instala√ß√£o e uso
   - FAQ e troubleshooting

8. **QUICKSTART.md** (2.7 KB)
   - Guia r√°pido para come√ßar
   - 3 op√ß√µes de uso
   - Dicas e otimiza√ß√µes

9. **RESUMO_EXECUTIVO.md** (8 KB)
   - Sum√°rio executivo para o orientador
   - Metodologia detalhada
   - Resultados esperados
   - Pr√≥ximos passos

10. **requirements.txt** (373 B)
    - Todas as depend√™ncias necess√°rias

---

## üöÄ Como Come√ßar

### Passo 1: Baixar Arquivos

Todos os arquivos est√£o em `/mnt/user-data/outputs/llm_annotation_system/`

### Passo 2: Instalar Depend√™ncias

```bash
pip install -r requirements.txt
```

### Passo 3: Configurar API Keys

Voc√™ precisa de API keys para:
- OpenAI (GPT-4, GPT-3.5)
- Anthropic (Claude 3)
- Google (Gemini)

Crie um arquivo `.env`:
```env
OPENAI_API_KEY=sua-key
ANTHROPIC_API_KEY=sua-key
GOOGLE_API_KEY=sua-key
```

### Passo 4: Executar

**RECOMENDADO**: Use o notebook Jupyter

```bash
jupyter notebook analise_consenso_llms.ipynb
```

---

## üìä O Que o Sistema Faz

### 1. Anota√ß√£o Autom√°tica

- ‚úÖ 5 LLMs anotam cada texto
- ‚úÖ Cada LLM anota 3x (valida√ß√£o interna)
- ‚úÖ Total: 15 anota√ß√µes por inst√¢ncia
- ‚úÖ Sistema de cache (n√£o repete chamadas)

### 2. An√°lise de Consenso

- ‚úÖ Calcula consenso entre LLMs
- ‚úÖ Calcula consenso interno de cada LLM
- ‚úÖ Identifica casos problem√°ticos (2-2-1, etc.)
- ‚úÖ M√©tricas estat√≠sticas completas

### 3. Valida√ß√£o de Par√¢metros

- ‚úÖ Testa diferentes temperaturas
- ‚úÖ Testa diferentes top_p
- ‚úÖ Analisa impacto nas anota√ß√µes
- ‚úÖ "LLM hacking" para otimiza√ß√£o

### 4. Visualiza√ß√µes

- ‚úÖ Heatmap de concord√¢ncia
- ‚úÖ Distribui√ß√£o de consenso
- ‚úÖ Matriz de confus√£o
- ‚úÖ Compara√ß√£o de modelos
- ‚úÖ Dashboard interativo

### 5. Outputs

- ‚úÖ CSVs com todas as anota√ß√µes
- ‚úÖ CSVs com alta confian√ßa (consenso ‚â•80%)
- ‚úÖ CSVs com casos para revis√£o
- ‚úÖ JSON com sum√°rio estat√≠stico
- ‚úÖ PNGs com gr√°ficos
- ‚úÖ HTML com dashboard interativo

---

## üí° Dicas Importantes

### Para Come√ßar com Poucos Custos

1. **Use amostra pequena primeiro**
   - Teste com 10-20 textos
   - Valide que est√° funcionando
   - Depois escale

2. **Use modelos mais baratos**
   - Comece com: GPT-3.5, Claude Sonnet, Gemini
   - Depois adicione GPT-4 e Claude Opus

3. **Aproveite o cache**
   - Sistema salva respostas automaticamente
   - N√£o repete chamadas de API
   - Economiza muito dinheiro

### Para Melhorar Qualidade

1. **Ajuste os prompts** em `config.py`
   - Adicione exemplos (few-shot)
   - Teste Chain-of-Thought
   - Seja espec√≠fico nas instru√ß√µes

2. **Teste diferentes configura√ß√µes**
   - Use `test_param_variations=True`
   - Analise qual funciona melhor
   - Documente seus achados

3. **Analise casos problem√°ticos**
   - Arquivo `needs_human_review.csv`
   - Entenda por que n√£o h√° consenso
   - Ajuste prompts ou categorias

---

## üéì Para Apresentar ao Orientador

### Material Pronto

1. **RESUMO_EXECUTIVO.md**
   - Leia e customize conforme necess√°rio
   - Adicione resultados reais quando tiver

2. **analise_consenso_llms.ipynb**
   - Execute e gere os resultados
   - Salve com outputs vis√≠veis
   - Apresente este notebook

3. **Dashboard Interativo**
   - Em `results/figures/interactive_dashboard.html`
   - Abra no navegador
   - Mostre as visualiza√ß√µes

### Pontos para Discutir

1. **Metodologia implementada**
   - Multi-LLM com consenso
   - Valida√ß√£o interna
   - Estrat√©gias de resolu√ß√£o

2. **Quest√µes de pesquisa**
   - Threshold ideal de consenso?
   - O que fazer com casos 2-2-1?
   - Few-shot learning ajuda?

3. **Pr√≥ximos passos**
   - Validar com ground truth
   - Testar em dataset maior
   - Otimizar custos

4. **Publica√ß√£o**
   - Onde submeter?
   - Quando?
   - Colabora√ß√µes?

---

## ‚úâÔ∏è Email Sugerido para Celso e Washington

```
Assunto: Valida√ß√£o de Prompt para Anota√ß√£o Autom√°tica com LLMs

Ol√° Celso e Washington,

Estou desenvolvendo uma metodologia para anota√ß√£o autom√°tica de datasets 
usando m√∫ltiplas LLMs com an√°lise de consenso. Implementei um sistema 
completo que testa diferentes prompts e configura√ß√µes.

Poderiam revisar o prompt base que estou usando? Est√° no arquivo config.py, 
linha 18 (BASE_ANNOTATION_PROMPT). Quero garantir que estou seguindo as 
melhores pr√°ticas de prompt engineering para classifica√ß√£o de textos.

Principais pontos:
- Prompt zero-shot com instru√ß√µes claras
- Suporte para few-shot (adicionar exemplos)
- Chain-of-Thought para casos complexos

Agrade√ßo muito o feedback de voc√™s!

Abra√ßo,
Gabriel Catizani
```

---

## üîß Customiza√ß√µes Poss√≠veis

### 1. Adicionar Novos Modelos

Edite `config.py` e adicione em `LLM_CONFIGS`:

```python
"novo-modelo": {
    "provider": "openai",  # ou anthropic, google
    "model_name": "nome-exato-do-modelo",
    "default_params": {"temperature": 0.0, "max_tokens": 50},
}
```

### 2. Mudar Categorias

No notebook ou script:

```python
categories = ["Sua", "Lista", "De", "Categorias"]
```

### 3. Customizar Prompts

Edite `config.py`:

```python
BASE_ANNOTATION_PROMPT = """
Seu prompt customizado aqui
{text}
{categories}
"""
```

### 4. Ajustar Par√¢metros

Em `config.py` ‚Üí `EXPERIMENT_CONFIG`:

```python
"num_repetitions_per_llm": 5,  # Aumentar repeti√ß√µes
"consensus_threshold": 0.7,     # Mudar threshold
"no_consensus_strategy": "...", # Mudar estrat√©gia
```

---

## üìà Estimativa de Custos

### Dataset Pequeno (100 textos)

- 5 modelos √ó 3 repeti√ß√µes = 15 anota√ß√µes/texto
- Total: 1.500 chamadas de API
- **Custo estimado: $3-5**

### Dataset M√©dio (1.000 textos)

- Total: 15.000 chamadas de API
- Com cache: ~10.000 chamadas √∫nicas
- **Custo estimado: $30-50**

### Dataset Grande (10.000 textos)

- Total: 150.000 chamadas
- Com cache e otimiza√ß√µes: ~100.000
- **Custo estimado: $300-500**

**Dica**: Comece pequeno, valide a metodologia, depois escale.

---

## ‚úÖ Checklist de Valida√ß√£o

Antes de apresentar ao orientador:

- [ ] Instalei todas as depend√™ncias
- [ ] Configurei minhas API keys
- [ ] Executei o notebook com dataset de teste
- [ ] Gerei todas as visualiza√ß√µes
- [ ] Analisei os resultados
- [ ] Li o RESUMO_EXECUTIVO.md
- [ ] Customizei para meu caso espec√≠fico
- [ ] Documentei achados importantes
- [ ] Preparei perguntas para discuss√£o

---

## üéØ Pr√≥ximos Passos Sugeridos

### Curto Prazo (1-2 semanas)

1. Teste com seu dataset real (amostra pequena)
2. Valide que a metodologia faz sentido
3. Ajuste prompts e par√¢metros
4. Apresente resultados preliminares ao orientador

### M√©dio Prazo (1-2 meses)

1. Execute em dataset completo
2. Valide com ground truth
3. Compare diferentes estrat√©gias
4. Documente resultados para paper

### Longo Prazo (3-6 meses)

1. Escreva o paper
2. Prepare apresenta√ß√£o
3. Submeta para confer√™ncia/journal
4. Disponibilize c√≥digo open-source

---

## üìû Precisa de Ajuda?

Se tiver d√∫vidas:

1. Consulte o README.md
2. Veja exemplos no notebook
3. Analise o c√≥digo (bem comentado)
4. Teste com datasets pequenos primeiro

---

## üéâ Conclus√£o

Voc√™ agora tem um **sistema completo e profissional** para sua pesquisa. 
O c√≥digo √© modular, bem documentado, e pronto para apresenta√ß√£o acad√™mica.

**Boa sorte com sua pesquisa!** üöÄ

Voc√™ tem uma metodologia s√≥lida, implementa√ß√£o robusta, e material excelente 
para apresentar ao seu orientador e eventualmente publicar.

---

Gabriel Catizani, espero que este sistema atenda suas necessidades. Qualquer d√∫vida, 
√© s√≥ perguntar! üòä
