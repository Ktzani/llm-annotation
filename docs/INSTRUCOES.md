# üìù Instru√ß√µes de Uso do Sistema

## üéØ O Que Voc√™ Tem

Um **sistema completo e profissional** para anota√ß√£o autom√°tica com LLMs. O c√≥digo est√° bem estruturado, documentado e pronto para uso em pesquisa.

---

## üì¶ Estrutura do Projeto

### C√≥digo Principal

**src/config/** - Configura√ß√µes centralizadas
- `prompts.py` - Templates de prompts otimizados
- `llm_configs.py` - Configura√ß√£o de todos os modelos LLM
- `experiment.py` - Par√¢metros do experimento
- `evaluation.py` - M√©tricas de avalia√ß√£o
- `conflict_resolution.py` - Estrat√©gias de resolu√ß√£o de conflitos
- `dataset_config.py` - ‚≠ê Configura√ß√£o de datasets HuggingFace

**src/llm_annotation_system/** - C√≥digo principal
- `llm_annotator.py` - Classe principal para anota√ß√£o
- `consensus_analyzer.py` - An√°lise de consenso e m√©tricas


**src/utils/** - Utilitarios
- `data_loader.py` - 
- `visualizer.py` - Gera√ß√£o de visualiza√ß√µes e dashboards

### Scripts de Execu√ß√£o

- `src/main.py` - Exemplo b√°sico de uso
- `src/main_huggingface.py` - ‚≠ê Script principal com HuggingFace

### Notebooks

- `src/notebooks/analise_consenso_llms.ipynb` - ‚≠ê Notebook completo

### Documenta√ß√£o

- `docs/README.md` - Documenta√ß√£o t√©cnica completa
- `docs/RESUMO_EXECUTIVO.md` - Sum√°rio executivo
---

## üöÄ Como Come√ßar

### Passo 1: Instalar Depend√™ncias

```bash
poetry install
```

### Passo 2: Configurar API Keys

Crie arquivo `.env` na raiz:

```env
OPENAI_API_KEY=sua-key-openai
ANTHROPIC_API_KEY=sua-key-anthropic
GOOGLE_API_KEY=sua-key-google
```

### Passo 3: Escolher Modo de Uso

#### Op√ß√£o A: Com Datasets HuggingFace (RECOMENDADO)

```bash
# 1. Descobrir estrutura do seu dataset
poetry run python src/main_huggingface.py --modo descobrir --dataset waashk/seu-dataset

# 2. Configurar em src/config/dataset_config.py
# (use a sugest√£o gerada pelo comando acima)

# 3. Executar anota√ß√£o
poetry run python src/main_huggingface.py --modo basico
```

#### Op√ß√£o B: Com Dados Locais

```bash
# Executar exemplo b√°sico
poetry run python src/main.py
```

#### Op√ß√£o C: Notebook Jupyter

```bash
# Abrir notebook
poetry run jupyter notebook src/notebooks/analise_consenso_llms.ipynb
```

---

## üìä O Que o Sistema Faz

### 1. Anota√ß√£o Autom√°tica

- ‚úÖ M√∫ltiplas LLMs anotam cada texto
- ‚úÖ Cada LLM anota m√∫ltiplas vezes (valida√ß√£o interna)
- ‚úÖ Total: 15 anota√ß√µes por inst√¢ncia (5 LLMs √ó 3 repeti√ß√µes)
- ‚úÖ Sistema de cache (n√£o repete chamadas)

### 2. An√°lise de Consenso

- ‚úÖ Calcula consenso entre LLMs
- ‚úÖ Calcula consenso interno de cada LLM
- ‚úÖ Identifica casos problem√°ticos (2-2-1, empates, etc.)
- ‚úÖ M√©tricas estat√≠sticas completas (Cohen's Kappa, Fleiss', etc.)

### 3. Valida√ß√£o com Ground Truth (Opcional)

- ‚úÖ Se dataset tem labels, valida automaticamente
- ‚úÖ Calcula accuracy, precision, recall, F1
- ‚úÖ Gera classification report
- ‚úÖ Identifica categorias problem√°ticas

### 4. Valida√ß√£o de Par√¢metros

- ‚úÖ Testa diferentes temperaturas
- ‚úÖ Testa diferentes top_p
- ‚úÖ Analisa impacto nas anota√ß√µes
- ‚úÖ "LLM hacking" para otimiza√ß√£o

### 5. Visualiza√ß√µes

- ‚úÖ Heatmap de concord√¢ncia entre modelos
- ‚úÖ Distribui√ß√£o de consenso
- ‚úÖ Matriz de confus√£o
- ‚úÖ Compara√ß√£o de modelos
- ‚úÖ Dashboard interativo (HTML)

### 6. Outputs Gerados

**CSVs:**
- `dataset_anotado_final.csv` - Dataset final anotado
- `annotations_complete.csv` - Todas as anota√ß√µes detalhadas
- `high_confidence_annotations.csv` - Consenso ‚â• 80%
- `needs_human_review.csv` - Casos problem√°ticos
- `pairwise_agreement.csv` - Acordo entre pares de modelos
- `confusion_matrix.csv` - Matriz de confus√£o

**Visualiza√ß√µes:**
- `agreement_heatmap.png` - Heatmap de concord√¢ncia
- `consensus_distribution.png` - Distribui√ß√£o de consenso
- `model_comparison.png` - Compara√ß√£o de modelos
- `interactive_dashboard.html` - ‚≠ê Dashboard interativo

**Resumos:**
- `experiment_summary.json` - Estat√≠sticas completas

---

## ü§ó Usar Datasets do HuggingFace

### Fluxo Completo

#### 1. Descobrir Estrutura

```bash
poetry run python src/main_huggingface.py --modo descobrir --dataset waashk/seu-dataset
```

**Output:**
```
üìã Estrutura do dataset:
   Colunas: ['text', 'label', 'id']
   
üìù Primeiros 3 exemplos...

üí° Sugest√£o de configura√ß√£o:
"seu_dataset": {
    "path": "waashk/seu-dataset",
    "text_column": "text",
    ...
}
```

#### 2. Configurar Dataset

Edite `src/config/dataset_config.py`:

```python
HUGGINGFACE_DATASETS = {
    "meu_dataset": {
        "path": "waashk/nome-do-dataset",
        "text_column": "text",              # Da descoberta
        "label_column": "label",            # Opcional (para valida√ß√£o)
        "categories": None,                  # Extrair automaticamente
        "combine_splits": ["train", "test"], # Dataset completo!
        "sample_size": 100,                  # Come√ßar pequeno
        "description": "Descri√ß√£o do dataset"
    },
}
```

#### 3. Executar

```bash
poetry run python src/main_huggingface.py --modo basico
```

### Casos de Uso

**Dataset com Labels (Valida√ß√£o):**
```python
"dataset_validacao": {
    "path": "waashk/dataset-com-labels",
    "text_column": "text",
    "label_column": "label",  # ‚Üê Tem ground truth
    "categories": None,       # Extrair das labels
    "combine_splits": ["train", "test"],
    "sample_size": None,
}
```
**Resultado:** Sistema calcula accuracy automaticamente!

**Dataset sem Labels (Anota√ß√£o Pura):**
```python
"dataset_novo": {
    "path": "waashk/textos-novos",
    "text_column": "content",
    "label_column": None,     # ‚Üê Sem labels
    "categories": ["A", "B", "C"],  # ‚Üê Voc√™ define
    "split": "train",
    "sample_size": None,
}
```
**Resultado:** Apenas anota√ß√µes, sem valida√ß√£o

---

## üí° Dicas Importantes

### Para Reduzir Custos

1. **Sempre come√ßar com amostra pequena**
   ```python
   "sample_size": 100  # ‚Üê Validar antes de escalar
   ```

2. **Usar modelos mais baratos primeiro**
   - Teste com: GPT-3.5, Claude Sonnet, Gemini
   - Depois adicione: GPT-4, Claude Opus

3. **Aproveitar o cache**
   - Sistema salva respostas automaticamente
   - N√£o repete chamadas de API
   - Economiza ~40% em custos

### Para Melhorar Qualidade

1. **Ajustar prompts** em `src/config/prompts.py`
   - Adicione exemplos (few-shot learning)
   - Teste Chain-of-Thought para casos complexos
   - Seja espec√≠fico nas instru√ß√µes

2. **Testar diferentes configura√ß√µes**
   ```python
   df = annotator.annotate_dataset(
       texts=texts,
       test_param_variations=True  # ‚Üê Testa varia√ß√µes
   )
   ```

3. **Analisar casos problem√°ticos**
   - Arquivo `needs_human_review.csv`
   - Entenda por que n√£o h√° consenso
   - Ajuste prompts ou categorias conforme necess√°rio

### Para Datasets Grandes

1. **Processar em batches**
   ```python
   batch_size = 500
   for i in range(0, len(texts), batch_size):
       batch = texts[i:i+batch_size]
       # Processar batch...
   ```

2. **Usar cache eficientemente**
   - Cache fica em `data/.cache/huggingface/`
   - Datasets baixados uma vez ficam em cache

---

## üéì Material para Apresenta√ß√£o

### Arquivos Prontos

1. **docs/RESUMO_EXECUTIVO.md**
   - Sum√°rio executivo do projeto
   - Metodologia detalhada
   - Resultados esperados

2. **src/notebooks/analise_consenso_llms.ipynb**
   - Execute e gere os resultados
   - Salve com outputs vis√≠veis
   - Use para apresenta√ß√£o

3. **results/figures/interactive_dashboard.html**
   - Dashboard interativo
   - Abra no navegador
   - Demonstre as an√°lises

### Pontos para Discuss√£o

1. **Metodologia Implementada**
   - Multi-LLM com an√°lise de consenso
   - Valida√ß√£o interna por repeti√ß√µes
   - Estrat√©gias de resolu√ß√£o de conflitos

2. **Quest√µes de Pesquisa**
   - Qual threshold ideal de consenso?
   - Como lidar com casos 2-2-1?
   - Few-shot learning melhora resultados?
   - Qual configura√ß√£o de par√¢metros √© melhor?

3. **Resultados e Valida√ß√£o**
   - Compara√ß√£o com ground truth
   - An√°lise de concord√¢ncia entre modelos
   - Custos vs qualidade

4. **Pr√≥ximos Passos**
   - Validar em dataset maior
   - Otimizar custos
   - Preparar publica√ß√£o

---

## üîß Customiza√ß√µes

### Adicionar Novos Modelos

Em `src/config/llm_configs.py`:

```python
LLM_CONFIGS["novo-modelo"] = {
    "provider": "openai",  # ou "anthropic", "google"
    "model_name": "nome-exato-do-modelo",
    "default_params": {
        "temperature": 0.0,
        "max_tokens": 50,
    },
    "alternative_params": {
        "temperature": [0.0, 0.3, 0.5],  # Para testes
    }
}
```

### Customizar Prompts

Em `src/config/prompts.py`:

```python
BASE_ANNOTATION_PROMPT = """
Seu prompt customizado aqui.

**Text to classify:**
{text}

**Available Categories:**
{categories}
"""
```

### Ajustar Par√¢metros do Experimento

Em `src/config/experiment.py`:

```python
EXPERIMENT_CONFIG = {
    "num_repetitions_per_llm": 5,      # Mais repeti√ß√µes
    "consensus_threshold": 0.7,         # Threshold diferente
    "test_param_variations": True,      # Testar varia√ß√µes
}
```

---

## üìà Estimativa de Custos

| Dataset | Chamadas API | Custo Estimado |
|---------|--------------|----------------|
| 100 textos | ~1.500 | $3-5 |
| 1.000 textos | ~15.000 | $30-50 |
| 10.000 textos | ~150.000 | $300-500 |

**Com cache e otimiza√ß√µes:** Redu√ß√£o de ~40%

**Dica:** Comece pequeno, valide metodologia, depois escale.

---

## ‚úÖ Checklist

Antes de executar em produ√ß√£o:

- [ ] Depend√™ncias instaladas (`poetry install`)
- [ ] API keys configuradas no `.env`
- [ ] Dataset estruturado ou configurado (`dataset_config.py`)
- [ ] Testado com amostra pequena (`sample_size: 100`)
- [ ] Prompts revisados e otimizados
- [ ] Categorias bem definidas
- [ ] Resultados validados em amostra
- [ ] Entendido custos estimados
- [ ] Backup dos dados importantes

---

