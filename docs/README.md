# Sistema de AnotaÃ§Ã£o AutomÃ¡tica com MÃºltiplas LLMs

Sistema completo para reduzir custos humanos na anotaÃ§Ã£o de datasets atravÃ©s do uso de mÃºltiplas LLMs e anÃ¡lise de consenso.

## ğŸ“‹ Ãndice

- [VisÃ£o Geral](#visÃ£o-geral)
- [Metodologia](#metodologia)
- [InstalaÃ§Ã£o](#instalaÃ§Ã£o)
- [Uso RÃ¡pido](#uso-rÃ¡pido)
- [Estrutura do Projeto](#estrutura-do-projeto)
- [Guia Detalhado](#guia-detalhado)
- [Resultados e MÃ©tricas](#resultados-e-mÃ©tricas)
- [FAQ](#faq)

## ğŸ¯ VisÃ£o Geral

Este projeto implementa uma metodologia para anotaÃ§Ã£o automÃ¡tica de datasets usando mÃºltiplas LLMs (Large Language Models) com anÃ¡lise de consenso. O objetivo Ã© reduzir significativamente o custo e tempo necessÃ¡rios para anotaÃ§Ã£o manual, mantendo alta qualidade nas classificaÃ§Ãµes.

### CaracterÃ­sticas Principais

âœ… **MÃºltiplas LLMs**: Suporte para GPT-4, GPT-3.5, Claude 3, Gemini, e Cohere  
âœ… **Consenso Robusto**: Cada LLM anota mÃºltiplas vezes para validaÃ§Ã£o interna  
âœ… **AnÃ¡lise EstatÃ­stica**: MÃ©tricas completas de concordÃ¢ncia (Cohen's Kappa, Fleiss' Kappa, etc.)  
âœ… **VisualizaÃ§Ãµes**: GrÃ¡ficos e dashboards interativos  
âœ… **FlexÃ­vel**: Suporte para diferentes estratÃ©gias de resoluÃ§Ã£o de conflitos  
âœ… **Cache**: Sistema de cache para economizar chamadas de API  

## ğŸ“Š Metodologia

A metodologia implementada segue os seguintes passos:

### 1. AnotaÃ§Ã£o MÃºltipla
- 5 LLMs diferentes anotam cada instÃ¢ncia do dataset
- Cada LLM faz mÃºltiplas anotaÃ§Ãµes (padrÃ£o: 3x) da mesma instÃ¢ncia
- ValidaÃ§Ã£o de consenso interno para cada LLM

### 2. AnÃ¡lise de Consenso
- CÃ¡lculo de tabela de consenso entre LLMs
- EstatÃ­sticas por instÃ¢ncia (porcentagem de acordo)
- IdentificaÃ§Ã£o de casos problemÃ¡ticos (empates, discordÃ¢ncias)

### 3. ValidaÃ§Ã£o de ParÃ¢metros (LLM Hacking)
- Teste de diferentes configuraÃ§Ãµes (temperatura, top_p, etc.)
- AvaliaÃ§Ã£o do impacto de variaÃ§Ãµes de parÃ¢metros
- IdentificaÃ§Ã£o de configuraÃ§Ãµes mais estÃ¡veis

### 4. EstratÃ©gias de ResoluÃ§Ã£o
Para casos sem consenso claro:
- **Voto majoritÃ¡rio**: Escolhe a classe mais votada
- **Threshold**: Aceita apenas se consenso â‰¥ X%
- **Flag for review**: Marca para revisÃ£o humana
- **Remove**: Remove instÃ¢ncias problemÃ¡ticas
- **Random**: SeleÃ§Ã£o aleatÃ³ria entre top 2

## ğŸš€ InstalaÃ§Ã£o

### PrÃ©-requisitos

- Python 3.8+
- API keys para as LLMs que deseja usar

### InstalaÃ§Ã£o de DependÃªncias

```bash
# Instalar dependÃªncias
pip install -r requirements.txt
```

### Configurar API Keys

Crie um arquivo `.env` na raiz do projeto:

```env
OPENAI_API_KEY=sua-chave-aqui
ANTHROPIC_API_KEY=sua-chave-aqui
GOOGLE_API_KEY=sua-chave-aqui
```

## ğŸƒ Uso RÃ¡pido

### OpÃ§Ã£o 1: Notebook Jupyter (RECOMENDADO)

```bash
jupyter notebook analise_consenso_llms.ipynb
```

Este notebook contÃ©m:
- Setup completo
- Exemplos de uso
- AnÃ¡lises detalhadas
- VisualizaÃ§Ãµes
- InterpretaÃ§Ã£o de resultados

### OpÃ§Ã£o 2: Script de Exemplo

```bash
python exemplo_uso.py
```

### OpÃ§Ã£o 3: Uso ProgramÃ¡tico

```python
from llm_annotator import LLMAnnotator
from consensus_analyzer import ConsensusAnalyzer

# Configurar
api_keys = {"openai": "...", "anthropic": "...", "google": "..."}
models = ["gpt-4-turbo", "claude-3-opus", "gemini-pro"]
categories = ["Positivo", "Negativo", "Neutro"]

# Anotar
annotator = LLMAnnotator(models, categories, api_keys)
df = annotator.annotate_dataset(texts, num_repetitions=3)
df = annotator.calculate_consensus(df)
```

## ğŸ“ Estrutura do Projeto

```
â”œâ”€â”€ config.py                      # ConfiguraÃ§Ãµes e prompts
â”œâ”€â”€ llm_annotator.py              # Classe principal de anotaÃ§Ã£o
â”œâ”€â”€ consensus_analyzer.py         # AnÃ¡lise de consenso
â”œâ”€â”€ visualizer.py                 # VisualizaÃ§Ãµes
â”œâ”€â”€ exemplo_uso.py                # Script de exemplo
â”œâ”€â”€ analise_consenso_llms.ipynb   # Notebook principal â­
â”œâ”€â”€ requirements.txt              # DependÃªncias
â””â”€â”€ README.md                     # Este arquivo
```

## ğŸ“– Guia Detalhado

### Customizar Prompts

Edite `config.py`:

```python
BASE_ANNOTATION_PROMPT = """You are an expert...
{text}
{categories}
"""
```

### Adicionar Novos Modelos

Em `config.py`:

```python
LLM_CONFIGS["seu-modelo"] = {
    "provider": "openai",
    "model_name": "nome-do-modelo",
    "default_params": {"temperature": 0.0}
}
```

### Testar VariaÃ§Ãµes de ParÃ¢metros

```python
df = annotator.annotate_dataset(
    texts=texts,
    test_param_variations=True  # Testa diferentes parÃ¢metros
)
```

## ğŸ“Š Resultados e MÃ©tricas

### InterpretaÃ§Ã£o

**Cohen's Kappa**:
- `> 0.80`: Excelente âœ…
- `0.60 - 0.80`: Bom âœ…
- `0.40 - 0.60`: Moderado âš ï¸
- `< 0.40`: Fraco âŒ

**Consenso Score**:
- `â‰¥ 80%`: Alto - aceitar âœ…
- `60-80%`: MÃ©dio - revisar amostra âš ï¸
- `< 60%`: Baixo - revisÃ£o obrigatÃ³ria âŒ

### Arquivos Gerados

1. **annotated_dataset_complete.csv**: Todas anotaÃ§Ãµes
2. **high_confidence_annotations.csv**: Consenso â‰¥ 80%
3. **needs_human_review.csv**: Casos problemÃ¡ticos
4. **experiment_summary.json**: EstatÃ­sticas

## ğŸ’¡ FAQ

**Q: Quantos modelos usar?**  
A: Recomendamos 5 modelos de diferentes provedores.

**Q: Quantas repetiÃ§Ãµes?**  
A: 3 repetiÃ§Ãµes Ã© um bom balanÃ§o entre confiabilidade e custo.

**Q: Como reduzir custos?**  
A: Use cache, amostras pequenas, e modelos mais baratos inicialmente.

**Q: O que fazer com casos sem consenso?**  
A: Depende do caso - revisÃ£o humana Ã© mais confiÃ¡vel, voto majoritÃ¡rio Ã© mais rÃ¡pido.

## ğŸ“§ PrÃ³ximos Passos

1. [ ] Configurar API keys em `.env`
2. [ ] Preparar seu dataset
3. [ ] Abrir o notebook `analise_consenso_llms.ipynb`
4. [ ] Seguir o passo a passo no notebook
5. [ ] Analisar resultados e visualizaÃ§Ãµes
6. [ ] Apresentar para orientador

## ğŸ™ Agradecimentos

Orientador e colaboradores: Marcos, Celso, Washington

---

**â­ Boa sorte com sua pesquisa!**
